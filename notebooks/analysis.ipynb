{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnxDlvP8szOj"
   },
   "source": [
    "# Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-9BelQPaJVo2"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9AH-GlrfS4Pf"
   },
   "outputs": [],
   "source": [
    "VALIDATION_DATASETS = [\"imagenet\", \"imagenette\", \"imagewoof\"]\n",
    "RESNET50_MODELS = [\n",
    "    \"resnet50\",\n",
    "    \"mocov3_resnet50\",\n",
    "    \"vicreg_resnet50\",\n",
    "    \"dino_resnet50\",\n",
    "    \"clip_RN50\",\n",
    "]\n",
    "VITB16_MODELS = [\n",
    "    \"vitb16\",\n",
    "    \"mocov3_vit_base\",\n",
    "    \"timm_vit_base_patch16_224.mae\",\n",
    "    \"dino_vitb16\",\n",
    "    \"clip_vitb16\",\n",
    "]\n",
    "CLUSTERERS = [\n",
    "    \"KMeans\",\n",
    "    \"AgglomerativeClustering\",\n",
    "    \"AffinityPropagation\",\n",
    "    \"SpectralClustering\",\n",
    "    \"HDBSCAN\",\n",
    "    \"OPTICS\",\n",
    "]\n",
    "ALL_CLUSTERERS = copy.deepcopy(CLUSTERERS)\n",
    "DISTANCE_METRICS = [\n",
    "    \"euclidean\",\n",
    "    \"l1\",\n",
    "    \"chebyshev\",\n",
    "    \"cosine\",\n",
    "    \"arccos\",\n",
    "    \"braycurtis\",\n",
    "    \"canberra\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0mJrqF3FAbEG"
   },
   "outputs": [],
   "source": [
    "DATASET2LS = {\n",
    "    \"imagenet\": \"-.\",\n",
    "    \"imagenette\": \"--\",\n",
    "    \"imagewoof\": \":\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OqPoCd4kbokG"
   },
   "outputs": [],
   "source": [
    "DEFAULT_PARAMS = {\n",
    "    \"all\": {\n",
    "        \"dim_reducer\": \"None\",\n",
    "        \"dim_reducer_man\": \"None\",\n",
    "        \"zscore\": False,\n",
    "        \"normalize\": False,\n",
    "        \"zscore2\": False,\n",
    "        \"ndim_correction\": False,\n",
    "    },\n",
    "    \"KMeans\": {\"clusterer\": \"KMeans\"},\n",
    "    \"AffinityPropagation\": {\n",
    "        \"clusterer\": \"AffinityPropagation\",\n",
    "        \"affinity_damping\": 0.5,\n",
    "        \"affinity_conv_iter\": 15,\n",
    "    },\n",
    "    \"SpectralClustering\": {\n",
    "        \"clusterer\": \"SpectralClustering\",\n",
    "        \"spectral_assigner\": \"kmeans\",\n",
    "    },\n",
    "    \"AgglomerativeClustering\": {\n",
    "        \"clusterer\": \"AgglomerativeClustering\",\n",
    "        \"distance_metric\": \"euclidean\",\n",
    "        \"aggclust_linkage\": \"ward\",\n",
    "    },\n",
    "    \"HDBSCAN\": {\n",
    "        \"clusterer\": \"HDBSCAN\",\n",
    "        \"hdbscan_method\": \"eom\",\n",
    "        \"min_samples\": 5,\n",
    "        \"max_samples\": 0.2,\n",
    "        \"distance_metric\": \"euclidean\",\n",
    "    },\n",
    "    \"OPTICS\": {\n",
    "        \"clusterer\": \"OPTICS\",\n",
    "        \"optics_method\": \"xi\",\n",
    "        \"optics_xi\": 0.05,\n",
    "        \"distance_metric\": \"euclidean\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSVZM4cns-gm"
   },
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8w21wu6JUk4d"
   },
   "outputs": [],
   "source": [
    "import matplotlib.colors\n",
    "\n",
    "def categorical_cmap(nc, nsc, cmap=\"tab10\", continuous=False):\n",
    "    \"\"\"\n",
    "    Create a colormap with a certain number of shades of colours.\n",
    "\n",
    "    https://stackoverflow.com/a/47232942/1960959\n",
    "    \"\"\"\n",
    "    if nc > plt.get_cmap(cmap).N:\n",
    "        raise ValueError(\"Too many categories for colormap.\")\n",
    "    if continuous:\n",
    "        ccolors = plt.get_cmap(cmap)(np.linspace(0, 1, nc))\n",
    "    else:\n",
    "        ccolors = plt.get_cmap(cmap)(np.arange(nc, dtype=int))\n",
    "    cols = np.zeros((nc * nsc, 3))\n",
    "    for i, c in enumerate(ccolors):\n",
    "        chsv = matplotlib.colors.rgb_to_hsv(c[:3])\n",
    "        arhsv = np.tile(chsv, nsc).reshape(nsc, 3)\n",
    "        arhsv[:, 1] = np.linspace(chsv[1], 0.25, nsc)\n",
    "        arhsv[:, 2] = np.linspace(chsv[2], 1, nsc)\n",
    "        rgb = matplotlib.colors.hsv_to_rgb(arhsv)\n",
    "        cols[i * nsc : (i + 1) * nsc, :] = rgb\n",
    "    cmap = matplotlib.colors.ListedColormap(cols)\n",
    "    return cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "id": "Zt-xAviSUwWV",
    "outputId": "8ce6d6f0-2a48-4aa8-a889-d669b0048fae"
   },
   "outputs": [],
   "source": [
    "categorical_cmap(len(RESNET50_MODELS), len(VALIDATION_DATASETS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i5hY2oc2I-PO"
   },
   "outputs": [],
   "source": [
    "def select_rows(df, filters, allow_missing=True):\n",
    "    select = np.ones(len(df), dtype=bool)\n",
    "    for col, val in filters.items():\n",
    "        if col == \"dataset\":\n",
    "            col = \"dataset_name\"\n",
    "        if col == \"clusterer\":\n",
    "            col = \"clusterer_name\"\n",
    "        if val is None or val == \"None\":\n",
    "            select_i = pd.isna(df[col])\n",
    "            select_i |= df[col] == \"None\"\n",
    "        else:\n",
    "            select_i = df[col] == val\n",
    "            select_i |= df[col] == str(val)\n",
    "            if allow_missing or val == \"None\":\n",
    "                select_i |= pd.isna(df[col])\n",
    "        select &= select_i\n",
    "    return df[select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7iOVrUnC_Jg-"
   },
   "outputs": [],
   "source": [
    "def find_differing_columns(df, cols=None):\n",
    "    if cols is None:\n",
    "        cols = df.columns\n",
    "    my_cols = []\n",
    "    for col in cols:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        if df[col].nunique(dropna=False) > 1:\n",
    "            my_cols.append(col)\n",
    "    return my_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wI7MkSQiz89O"
   },
   "outputs": [],
   "source": [
    "def filter2command(*filters, partition=\"val\"):\n",
    "    f = {}\n",
    "    for filter in filters:\n",
    "        for k, v in filter.items():\n",
    "            f[k] = v\n",
    "    dataset = f.get(\"dataset\", \"\")\n",
    "    clusterer = f.get(\"clusterer\", \"\")\n",
    "    mem = 4\n",
    "    if dataset != \"imagenet\":\n",
    "        pass\n",
    "    elif clusterer == \"AgglomerativeClustering\":\n",
    "        mem = 20\n",
    "    if partition == \"val\":\n",
    "        seed = 100\n",
    "    elif partition == \"test\":\n",
    "        seed = 1\n",
    "    else:\n",
    "        seed = 0\n",
    "    s = (\n",
    "        f\"sbatch --array={seed} --mem={mem}G\"\n",
    "        f' --job-name=\"zsc-{f.get(\"model\", \"\")}-{dataset}-{clusterer}\"'\n",
    "        f\" slurm/cluster.slrm --partition={partition}\"\n",
    "    )\n",
    "    for k, v in f.items():\n",
    "        if v is None:\n",
    "            continue\n",
    "        if k == \"zscore\":\n",
    "            if v == \"False\" or not v:\n",
    "                s += \" --no-zscore\"\n",
    "            elif v == \"True\" or v:\n",
    "                s += \" --zscore\"\n",
    "            continue\n",
    "        if k == \"normalize\":\n",
    "            if v == \"False\" or not v:\n",
    "                pass\n",
    "            elif v == \"True\" or v:\n",
    "                s += \" --normalize\"\n",
    "            continue\n",
    "        if k == \"zscore2\":\n",
    "            if v == \"False\" or not v:\n",
    "                s += \" --no-zscore2\"\n",
    "            elif v == \"average\":\n",
    "                s += \" --azscore2\"\n",
    "            elif v == \"standard\" or v:\n",
    "                s += \" --zscore2\"\n",
    "            continue\n",
    "        if k == \"ndim_correction\":\n",
    "            if v == \"False\" or not v:\n",
    "                s += \" --no-ndim-correction\"\n",
    "            elif v == \"True\" or v:\n",
    "                s += \" --ndim-correction\"\n",
    "            continue\n",
    "        s += f\" --{k.replace('_', '-')}={v}\"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mViY7L8lcked"
   },
   "source": [
    "# Hyperparameter searches on val data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opMRar5wtQSx"
   },
   "source": [
    "### Fetch results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PuCnc-t9rQ4q"
   },
   "outputs": [],
   "source": [
    "# Project is specified by <entity/project-name>\n",
    "api = wandb.Api()\n",
    "runs = api.runs(\n",
    "    \"uoguelph_mlrg/zs-ssl-clustering\",\n",
    "    filters={\"state\": \"Finished\", \"config.partition\": \"val\"},\n",
    ")\n",
    "len(runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZz6EautFlEa"
   },
   "outputs": [],
   "source": [
    "summary_list, config_list, name_list = [], [], []\n",
    "for run in runs:\n",
    "    # .summary contains the output keys/values for metrics like accuracy.\n",
    "    #  We call ._json_dict to omit large files\n",
    "    summary_list.append(run.summary._json_dict)\n",
    "    # .config contains the hyperparameters.\n",
    "    #  We remove special values that start with _.\n",
    "    config_list.append({k: v for k, v in run.config.items() if not k.startswith(\"_\")})\n",
    "    # .name is the human-readable name of the run.\n",
    "    name_list.append(run.name)\n",
    "\n",
    "runs_df = pd.DataFrame(\n",
    "    {\"summary\": summary_list, \"config\": config_list, \"name\": name_list}\n",
    ")\n",
    "\n",
    "rows = []\n",
    "config_keys = set()\n",
    "summary_keys = set()\n",
    "for summary, config, name in zip(summary_list, config_list, name_list):\n",
    "    row = {\"name\": name}\n",
    "    row.update({k: v for k, v in config.items() if not k.startswith(\"_\")})\n",
    "    row.update({k: v for k, v in summary.items() if not k.startswith(\"_\")})\n",
    "    if \"_timestamp\" in summary:\n",
    "        row[\"_timestamp\"] = summary[\"_timestamp\"]\n",
    "    rows.append(row)\n",
    "    config_keys = config_keys.union(config.keys())\n",
    "    summary_keys = summary_keys.union(summary.keys())\n",
    "\n",
    "runs_df = pd.DataFrame.from_records(rows)\n",
    "print(len(runs_df))\n",
    "\n",
    "# Handle changed default value for spectral_assigner after config arg was introduced\n",
    "if \"spectral_assigner\" not in runs_df.columns:\n",
    "    runs_df[\"spectral_assigner\"] = None\n",
    "select = runs_df[\"clusterer_name\"] != \"SpectralClustering\"\n",
    "runs_df.loc[select, \"spectral_assigner\"] = None\n",
    "select = (runs_df[\"clusterer_name\"] == \"SpectralClustering\") & pd.isna(\n",
    "    runs_df[\"spectral_assigner\"]\n",
    ")\n",
    "runs_df.loc[select, \"spectral_assigner\"] = \"kmeans\"\n",
    "\n",
    "if \"zscore2\" not in runs_df.columns:\n",
    "    runs_df[\"zscore2\"] = False\n",
    "runs_df.loc[pd.isna(runs_df[\"zscore2\"]), \"zscore2\"] = False\n",
    "\n",
    "if \"ndim_correction\" not in runs_df.columns:\n",
    "    runs_df[\"ndim_correction\"] = False\n",
    "runs_df.loc[pd.isna(runs_df[\"ndim_correction\"]), \"ndim_correction\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dQW4p_22YL69"
   },
   "outputs": [],
   "source": [
    "config_keys = config_keys.difference(\n",
    "    {\"workers\", \"memory_avail_GB\", \"memory_total_GB\", \"memory_slurm\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mFRtnzmOH-6f"
   },
   "outputs": [],
   "source": [
    "# Remove entries without an AMI metric\n",
    "runs_df = runs_df[~runs_df[\"AMI\"].isna()]\n",
    "len(runs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d1R2eJtxHrK-"
   },
   "outputs": [],
   "source": [
    "runs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H8v330NeKd5T"
   },
   "outputs": [],
   "source": [
    "config_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VrIuBBBcKfeh"
   },
   "outputs": [],
   "source": [
    "summary_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxLWvsoItB6p"
   },
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XfJUeBenJuUj"
   },
   "outputs": [],
   "source": [
    "sdf = select_rows(\n",
    "    runs_df,\n",
    "    {\n",
    "        \"model\": \"resnet50\",\n",
    "        \"dataset\": \"imagenette\",\n",
    "        \"clusterer\": \"KMeans\",\n",
    "        \"dim_reducer\": \"PCA\",\n",
    "    },\n",
    "    allow_missing=False,\n",
    ")\n",
    "sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mDfUksgsLUON"
   },
   "outputs": [],
   "source": [
    "sdf[\"reduced_dim\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2tiDbsLBQ7RP"
   },
   "outputs": [],
   "source": [
    "sdf = sdf.sort_values(\"reduced_dim\")\n",
    "plt.plot(sdf[\"reduced_dim\"], sdf[\"AMI\"])\n",
    "plt.xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RbueTcNgI0fJ"
   },
   "outputs": [],
   "source": [
    "plt.plot(sdf[\"pca_explained_ratio\"], sdf[\"AMI\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6tNLcKT-R_5d"
   },
   "outputs": [],
   "source": [
    "sdf = sdf.sort_values(\"reduced_dim\")\n",
    "plt.plot(sdf[\"reduced_dim\"], sdf[\"pca_explained_ratio\"])\n",
    "plt.xlabel(\"Dimension\")\n",
    "plt.ylabel(\"Variance explained\")\n",
    "plt.title(\"imagenette: resnet50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-jCRw79VB-2"
   },
   "outputs": [],
   "source": [
    "cmap = categorical_cmap(len(RESNET50_MODELS), len(VALIDATION_DATASETS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mAHv_cRkVCp6"
   },
   "outputs": [],
   "source": [
    "cmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8jc_xmzHc95"
   },
   "source": [
    "## PCA dim vs variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "012WFlAoBhU1"
   },
   "outputs": [],
   "source": [
    "models = RESNET50_MODELS + VITB16_MODELS\n",
    "cmap = categorical_cmap(len(models), len(VALIDATION_DATASETS))\n",
    "clusterer = \"KMeans\"\n",
    "plt.figure(figsize=(10, 8))\n",
    "i = 0\n",
    "for model in models:\n",
    "    for dataset in VALIDATION_DATASETS:\n",
    "        filter = {\n",
    "            \"model\": model,\n",
    "            \"dataset\": dataset,\n",
    "            \"clusterer\": clusterer,\n",
    "            \"dim_reducer\": \"PCA\",\n",
    "            \"zscore\": True,\n",
    "        }\n",
    "        sdf = select_rows(runs_df, filter, allow_missing=False)\n",
    "        filter2 = dict(DEFAULT_PARAMS[\"all\"], **DEFAULT_PARAMS[clusterer])\n",
    "        filter2 = {k: v for k, v in filter2.items() if k not in filter}\n",
    "        sdf = select_rows(sdf, filter2, allow_missing=True)\n",
    "        sdf = sdf.sort_values(\"reduced_dim\")\n",
    "        plt.plot(\n",
    "            sdf[\"reduced_dim\"],\n",
    "            sdf[\"pca_explained_ratio\"],\n",
    "            label=f\"{dataset}: {model}\",\n",
    "            c=cmap(i),\n",
    "        )\n",
    "        i += 1\n",
    "\n",
    "plt.xlabel(\"Number of dimensions\")\n",
    "plt.ylabel(\"Cummulative variance fraction explained\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pq5_sRpnHsWh"
   },
   "source": [
    "## Dim count (loose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eDCL777RSuyg"
   },
   "outputs": [],
   "source": [
    "models = RESNET50_MODELS + VITB16_MODELS\n",
    "cmap = categorical_cmap(len(models), len(VALIDATION_DATASETS))\n",
    "for clusterer in CLUSTERERS:\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    i = 0\n",
    "    for model in models:\n",
    "        for dataset in VALIDATION_DATASETS:\n",
    "            filter = {\n",
    "                \"model\": model,\n",
    "                \"dataset\": dataset,\n",
    "                \"clusterer\": clusterer,\n",
    "                \"dim_reducer\": \"PCA\",\n",
    "                \"zscore\": True,\n",
    "            }\n",
    "            sdf = select_rows(runs_df, filter, allow_missing=False)\n",
    "            filter2 = dict(DEFAULT_PARAMS[\"all\"], **DEFAULT_PARAMS[clusterer])\n",
    "            filter2 = {k: v for k, v in filter2.items() if k not in filter}\n",
    "            sdf = select_rows(sdf, filter2, allow_missing=True)\n",
    "            sdf = sdf.sort_values(\"reduced_dim\")\n",
    "            plt.plot(\n",
    "                sdf[\"reduced_dim\"],\n",
    "                sdf[\"AMI\"],\n",
    "                label=f\"{dataset}: {model}\",\n",
    "                c=cmap(i),\n",
    "            )\n",
    "            i += 1\n",
    "\n",
    "    plt.xlabel(\"Dimension\")\n",
    "    plt.ylabel(\"AMI\")\n",
    "    plt.title(clusterer)\n",
    "    plt.xscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FDw-t2JiY94W"
   },
   "outputs": [],
   "source": [
    "models = RESNET50_MODELS + VITB16_MODELS\n",
    "cmap = categorical_cmap(len(models), len(VALIDATION_DATASETS))\n",
    "for clusterer in CLUSTERERS:\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    i = 0\n",
    "    for model in models:\n",
    "        for dataset in VALIDATION_DATASETS:\n",
    "            filter = {\n",
    "                \"model\": model,\n",
    "                \"dataset\": dataset,\n",
    "                \"clusterer\": clusterer,\n",
    "                \"dim_reducer\": \"PCA\",\n",
    "                \"zscore\": True,\n",
    "            }\n",
    "            sdf = select_rows(runs_df, filter, allow_missing=False)\n",
    "            filter2 = dict(DEFAULT_PARAMS[\"all\"], **DEFAULT_PARAMS[clusterer])\n",
    "            filter2 = {k: v for k, v in filter2.items() if k not in filter}\n",
    "            sdf = select_rows(sdf, filter2, allow_missing=True)\n",
    "            sdf = sdf.sort_values(\"pca_explained_ratio\")\n",
    "            plt.plot(\n",
    "                sdf[\"pca_explained_ratio\"],\n",
    "                sdf[\"AMI\"],\n",
    "                label=f\"{dataset}: {model}\",\n",
    "                c=cmap(i),\n",
    "            )\n",
    "            i += 1\n",
    "\n",
    "    plt.xlabel(\"Kept variance explained ratio\")\n",
    "    plt.ylabel(\"AMI\")\n",
    "    plt.title(clusterer)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GLjhrk5MXdi-"
   },
   "outputs": [],
   "source": [
    "models = RESNET50_MODELS + VITB16_MODELS\n",
    "cmap = categorical_cmap(len(models), len(VALIDATION_DATASETS))\n",
    "for clusterer in CLUSTERERS:\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    i = 0\n",
    "    for model in models:\n",
    "        for dataset in VALIDATION_DATASETS:\n",
    "            filter = {\n",
    "                \"model\": model,\n",
    "                \"dataset\": dataset,\n",
    "                \"clusterer\": clusterer,\n",
    "                \"dim_reducer_man\": \"UMAP\",\n",
    "            }\n",
    "            sdf = select_rows(runs_df, filter, allow_missing=False)\n",
    "            sdf = select_rows(\n",
    "                sdf, {\"dim_reducer_man_metric\": \"euclidean\"}, allow_missing=True\n",
    "            )\n",
    "            filter2 = dict(DEFAULT_PARAMS[\"all\"], **DEFAULT_PARAMS[clusterer])\n",
    "            filter2 = {k: v for k, v in filter2.items() if k not in filter}\n",
    "            sdf = select_rows(sdf, filter2, allow_missing=True)\n",
    "            sdf = sdf.sort_values(\"reduced_dim\")\n",
    "            plt.plot(\n",
    "                sdf[\"reduced_dim\"],\n",
    "                sdf[\"AMI\"],\n",
    "                label=f\"{dataset}: {model}\",\n",
    "                c=cmap(i),\n",
    "            )\n",
    "            i += 1\n",
    "\n",
    "    plt.xlabel(\"Dimensions\")\n",
    "    plt.ylabel(\"AMI\")\n",
    "    plt.title(clusterer)\n",
    "    plt.xscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fPG1YvjGGunq"
   },
   "source": [
    "## HDBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53SmbpjaHBog"
   },
   "source": [
    "### Max samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wvAANYhkezsN"
   },
   "outputs": [],
   "source": [
    "models = RESNET50_MODELS + VITB16_MODELS\n",
    "cmap = categorical_cmap(len(models), len(VALIDATION_DATASETS))\n",
    "clusterer = \"HDBSCAN\"\n",
    "plt.figure(figsize=(10, 8))\n",
    "i = -1\n",
    "for model in models:\n",
    "    for dataset in VALIDATION_DATASETS:\n",
    "        i += 1\n",
    "        filter = {\n",
    "            \"model\": model,\n",
    "            \"dataset\": dataset,\n",
    "            \"clusterer\": clusterer,\n",
    "        }\n",
    "        sdf = select_rows(runs_df, filter, allow_missing=False)\n",
    "        filter2 = dict(DEFAULT_PARAMS[\"all\"], **DEFAULT_PARAMS[clusterer])\n",
    "        filter2 = {k: v for k, v in filter2.items() if k not in filter}\n",
    "        filter2 = {k: v for k, v in filter2.items() if k not in [\"max_samples\"]}\n",
    "        sdf = select_rows(sdf, filter2, allow_missing=True)\n",
    "        sdf = sdf.sort_values(\"max_samples\")\n",
    "        if len(sdf) > 0 and sum(~pd.isna(sdf[\"max_samples\"])) > 0:\n",
    "            plt.plot(\n",
    "                sdf[\"max_samples\"],\n",
    "                sdf[\"AMI\"],\n",
    "                label=f\"{dataset}: {model}\",\n",
    "                c=cmap(i),\n",
    "            )\n",
    "\n",
    "plt.xlabel(\"Max samples per cluster\")\n",
    "plt.ylabel(\"AMI\")\n",
    "plt.title(clusterer)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vS4LHHiHKCG"
   },
   "source": [
    "### EOM vs leaf and distance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kajP6Vzag_PI"
   },
   "outputs": [],
   "source": [
    "cmap = categorical_cmap(len(RESNET50_MODELS), len(VALIDATION_DATASETS))\n",
    "clusterer = \"HDBSCAN\"\n",
    "methods = [\"eom\", \"leaf\"]\n",
    "metrics = DISTANCE_METRICS\n",
    "\n",
    "data = np.NaN * np.ones(\n",
    "    (len(RESNET50_MODELS), len(VALIDATION_DATASETS), len(methods), len(metrics))\n",
    ")\n",
    "cmds = []\n",
    "for i_model, model in enumerate(RESNET50_MODELS):\n",
    "    for i_dataset, dataset in enumerate(VALIDATION_DATASETS):\n",
    "        for i_method, method in enumerate(methods):\n",
    "            for i_metric, metric in enumerate(metrics):\n",
    "                if metric == \"cosine\":\n",
    "                    continue\n",
    "                filter = {\n",
    "                    \"model\": model,\n",
    "                    \"dataset\": dataset,\n",
    "                    \"clusterer\": clusterer,\n",
    "                    \"distance_metric\": metric,\n",
    "                    \"hdbscan_method\": method,\n",
    "                }\n",
    "                if method == \"eom\":\n",
    "                    filter[\"max_samples\"] = 0.25\n",
    "                sdf = select_rows(runs_df, filter, allow_missing=False)\n",
    "                filter2 = dict(DEFAULT_PARAMS[\"all\"], **DEFAULT_PARAMS[clusterer])\n",
    "                filter2 = {k: v for k, v in filter2.items() if k not in filter}\n",
    "                sdf = select_rows(sdf, filter2, allow_missing=True)\n",
    "                if len(sdf) < 1:\n",
    "                    print(\"No data for\", filter)\n",
    "                    cmds.append(filter2command(filter, filter2))\n",
    "                    continue\n",
    "                if len(sdf) > 1:\n",
    "                    if sum(sdf[\"AMI\"] != sdf.iloc[0][\"AMI\"]) > 0:\n",
    "                        print()\n",
    "                        print(\"More than one result with AMIs:\", list(sdf[\"AMI\"]))\n",
    "                        print(f\"for search {filter} and {filter2}\")\n",
    "                        dif_cols = find_differing_columns(sdf, config_keys)\n",
    "                        print(f\"columns which differ: {dif_cols}\")\n",
    "                        print()\n",
    "                data[i_model, i_dataset, i_method, i_metric] = np.median(sdf[\"AMI\"])\n",
    "\n",
    "for cmd in cmds:\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-DMe3q3OGXj"
   },
   "outputs": [],
   "source": [
    "np.mean(np.mean(data, axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7DJLRKy4PSy-"
   },
   "outputs": [],
   "source": [
    "np.nanmean(np.nanmean(data, axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WNhRk0JUWbLi"
   },
   "outputs": [],
   "source": [
    "data[0, 0, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8CZNU7o4WSNZ"
   },
   "outputs": [],
   "source": [
    "max_data = np.nanmax(data)\n",
    "YLIM = [-0.05 * max_data, 1.05 * max_data]\n",
    "for i_method, method in enumerate(methods):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    ax = plt.axes()\n",
    "    i = 0\n",
    "    for i_model, model in enumerate(RESNET50_MODELS):\n",
    "        for i_dataset, dataset in enumerate(VALIDATION_DATASETS):\n",
    "            plt.plot(\n",
    "                data[i_model, i_dataset, i_method, :],\n",
    "                \"x\",\n",
    "                label=f\"{dataset}: {model}\",\n",
    "                c=cmap(i),\n",
    "            )\n",
    "            i += 1\n",
    "    # plt.legend()\n",
    "    ax.set_xticks(np.arange(len(metrics)), metrics)\n",
    "    plt.ylim(YLIM)\n",
    "    plt.title(method)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MjRD8fabYz-5"
   },
   "outputs": [],
   "source": [
    "width = 0.05\n",
    "max_data = np.nanmax(data)\n",
    "YLIM = [-0.05 * max_data, 1.05 * max_data]\n",
    "for i_method, method in enumerate(methods):\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    ax = plt.axes()\n",
    "    i = 0\n",
    "    for i_model, model in enumerate(RESNET50_MODELS):\n",
    "        for i_dataset, dataset in enumerate(VALIDATION_DATASETS):\n",
    "            plt.bar(\n",
    "                np.arange(len(metrics)) + i * width,\n",
    "                data[i_model, i_dataset, i_method, :],\n",
    "                width=width,\n",
    "                label=f\"{dataset}: {model}\",\n",
    "                color=cmap(i),\n",
    "            )\n",
    "            i += 1\n",
    "    # plt.legend()\n",
    "    ax.set_xticks(np.arange(len(metrics)) + width * (i + 1) / 2, metrics)\n",
    "    plt.ylim(YLIM)\n",
    "    plt.title(method)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtZPxRqiHRyQ"
   },
   "source": [
    "## Dimensionality selection (proper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8q8cgHRCL8VC"
   },
   "outputs": [],
   "source": [
    "CLUSTERERS = [\n",
    "    \"KMeans\",\n",
    "    \"AgglomerativeClustering\",\n",
    "    \"AffinityPropagation\",\n",
    "    \"SpectralClustering\",\n",
    "    \"HDBSCAN\",\n",
    "    \"OPTICS\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bI7z2uU7MNbD"
   },
   "outputs": [],
   "source": [
    "DEFAULT_PARAMS[\"HDBSCAN\"][\"max_samples\"] = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FSXafiTooibY"
   },
   "outputs": [],
   "source": [
    "models = RESNET50_MODELS + VITB16_MODELS\n",
    "axis_values = [2, 5, 10, 20, 50, 100, 200, 500]\n",
    "\n",
    "data_pca = np.NaN * np.ones(\n",
    "    (len(CLUSTERERS), len(models), len(VALIDATION_DATASETS), len(axis_values))\n",
    ")\n",
    "\n",
    "cmds = []\n",
    "for i_clusterer, clusterer in enumerate(CLUSTERERS):\n",
    "    for i_model, model in enumerate(models):\n",
    "        for i_dataset, dataset in enumerate(VALIDATION_DATASETS):\n",
    "            for i_value, axis_value in enumerate(axis_values):\n",
    "                filter = {\n",
    "                    \"model\": model,\n",
    "                    \"dataset\": dataset,\n",
    "                    \"clusterer\": clusterer,\n",
    "                    \"aggclust_dist_thresh\": None,\n",
    "                    \"dim_reducer\": \"PCA\",\n",
    "                    \"zscore\": True,\n",
    "                    \"ndim_reduced\": axis_value,\n",
    "                }\n",
    "                sdf = select_rows(runs_df, filter, allow_missing=False)\n",
    "                filter2 = dict(DEFAULT_PARAMS[\"all\"], **DEFAULT_PARAMS[clusterer])\n",
    "                filter2 = {k: v for k, v in filter2.items() if k not in filter}\n",
    "                sdf = select_rows(sdf, filter2, allow_missing=False)\n",
    "                if len(sdf) < 1:\n",
    "                    if dataset == \"imagenet\" and clusterer in [\n",
    "                        \"AffinityPropagation\",\n",
    "                        \"SpectralClustering\",\n",
    "                    ]:\n",
    "                        continue\n",
    "                    if clusterer in [\"SpectralClustering\"]:\n",
    "                        continue\n",
    "                    print(\"No data for\", filter)\n",
    "                    cmds.append(filter2command(filter, filter2))\n",
    "                    continue\n",
    "                if len(sdf) > 1:\n",
    "                    if sum(sdf[\"AMI\"] != sdf.iloc[0][\"AMI\"]) > 0:\n",
    "                        print()\n",
    "                        print(\"More than one result with AMIs:\", list(sdf[\"AMI\"]))\n",
    "                        print(f\"for search {filter} and {filter2}\")\n",
    "                        dif_cols = find_differing_columns(sdf, config_keys)\n",
    "                        print(f\"columns which differ: {dif_cols}\")\n",
    "                        print()\n",
    "                data_pca[i_clusterer, i_model, i_dataset, i_value] = np.median(\n",
    "                    sdf[\"AMI\"]\n",
    "                )\n",
    "\n",
    "for cmd in cmds:\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xtg_fWu4q3gT"
   },
   "outputs": [],
   "source": [
    "axis_values = pca_var_values = [0.75, 0.8, 0.85, 0.90, 0.95, 0.98, 0.99]\n",
    "\n",
    "data_pca_var = np.NaN * np.ones(\n",
    "    (len(CLUSTERERS), len(models), len(VALIDATION_DATASETS), len(axis_values))\n",
    ")\n",
    "\n",
    "cmds = []\n",
    "for i_clusterer, clusterer in enumerate(CLUSTERERS):\n",
    "    for i_model, model in enumerate(models):\n",
    "        for i_dataset, dataset in enumerate(VALIDATION_DATASETS):\n",
    "            for i_value, axis_value in enumerate(axis_values):\n",
    "                filter = {\n",
    "                    \"model\": model,\n",
    "                    \"dataset\": dataset,\n",
    "                    \"clusterer\": clusterer,\n",
    "                    \"aggclust_dist_thresh\": None,\n",
    "                    \"dim_reducer\": \"PCA\",\n",
    "                    \"zscore\": True,\n",
    "                    \"pca_variance\": axis_value,\n",
    "                }\n",
    "                sdf = select_rows(runs_df, filter, allow_missing=False)\n",
    "                filter2 = dict(DEFAULT_PARAMS[\"all\"], **DEFAULT_PARAMS[clusterer])\n",
    "                filter2 = {k: v for k, v in filter2.items() if k not in filter}\n",
    "                sdf = select_rows(sdf, filter2, allow_missing=False)\n",
    "                if len(sdf) < 1:\n",
    "                    if dataset == \"imagenet\" and clusterer in [\n",
    "                        \"AffinityPropagation\",\n",
    "                        \"SpectralClustering\",\n",
    "                    ]:\n",
    "                        continue\n",
    "                    if clusterer in [\"SpectralClustering\"]:\n",
    "                        continue\n",
    "                    print(\"No data for\", filter)\n",
    "                    cmds.append(filter2command(filter, filter2))\n",
    "                    continue\n",
    "                if len(sdf) > 1:\n",
    "                    if sum(sdf[\"AMI\"] != sdf.iloc[0][\"AMI\"]) > 0:\n",
    "                        print()\n",
    "                        print(\"More than one result with AMIs:\", list(sdf[\"AMI\"]))\n",
    "                        print(f\"for search {filter} and {filter2}\")\n",
    "                        dif_cols = find_differing_columns(sdf, config_keys)\n",
    "                        print(f\"columns which differ: {dif_cols}\")\n",
    "                        print()\n",
    "                data_pca_var[i_clusterer, i_model, i_dataset, i_value] = np.median(\n",
    "                    sdf[\"AMI\"]\n",
    "                )\n",
    "\n",
    "for cmd in cmds:\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GOla9Pio28Op"
   },
   "outputs": [],
   "source": [
    "axis_values = [2, 5, 10, 20, 50, 100, 200, 500]\n",
    "\n",
    "data_umap = np.NaN * np.ones(\n",
    "    (len(CLUSTERERS), len(models), len(VALIDATION_DATASETS), len(axis_values))\n",
    ")\n",
    "\n",
    "cmds = []\n",
    "for i_clusterer, clusterer in enumerate(CLUSTERERS):\n",
    "    for i_model, model in enumerate(models):\n",
    "        for i_dataset, dataset in enumerate(VALIDATION_DATASETS):\n",
    "            for i_value, axis_value in enumerate(axis_values):\n",
    "                filter = {\n",
    "                    \"model\": model,\n",
    "                    \"dataset\": dataset,\n",
    "                    \"clusterer\": clusterer,\n",
    "                    \"aggclust_dist_thresh\": None,\n",
    "                    \"dim_reducer_man\": \"UMAP\",\n",
    "                    \"ndim_reduced_man\": axis_value,\n",
    "                }\n",
    "                sdf = select_rows(runs_df, filter, allow_missing=False)\n",
    "                filter2 = dict(DEFAULT_PARAMS[\"all\"], **DEFAULT_PARAMS[clusterer])\n",
    "                filter2 = {k: v for k, v in filter2.items() if k not in filter}\n",
    "                sdf = select_rows(sdf, filter2, allow_missing=False)\n",
    "                if len(sdf) < 1:\n",
    "                    if dataset == \"imagenet\" and clusterer in [\n",
    "                        \"AffinityPropagation\",\n",
    "                        \"SpectralClustering\",\n",
    "                    ]:\n",
    "                        continue\n",
    "                    if clusterer in [\"SpectralClustering\"]:\n",
    "                        continue\n",
    "                    print(\"No data for\", filter)\n",
    "                    cmds.append(filter2command(filter, filter2))\n",
    "                    continue\n",
    "                if len(sdf) > 1:\n",
    "                    if sum(sdf[\"AMI\"] != sdf.iloc[0][\"AMI\"]) > 0:\n",
    "                        print()\n",
    "                        print(\"More than one result with AMIs:\", list(sdf[\"AMI\"]))\n",
    "                        print(f\"for search {filter} and {filter2}\")\n",
    "                        dif_cols = find_differing_columns(sdf, config_keys)\n",
    "                        print(f\"columns which differ: {dif_cols}\")\n",
    "                        print()\n",
    "                data_umap[i_clusterer, i_model, i_dataset, i_value] = np.median(\n",
    "                    sdf[\"AMI\"]\n",
    "                )\n",
    "\n",
    "for cmd in cmds:\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WkqTgKta9m78"
   },
   "outputs": [],
   "source": [
    "data_base = np.NaN * np.ones(\n",
    "    (len(CLUSTERERS), len(models), len(VALIDATION_DATASETS), 2)\n",
    ")\n",
    "\n",
    "cmds = []\n",
    "for i_clusterer, clusterer in enumerate(CLUSTERERS):\n",
    "    for i_model, model in enumerate(models):\n",
    "        for i_dataset, dataset in enumerate(VALIDATION_DATASETS):\n",
    "            # no z-score\n",
    "            filter = {\n",
    "                \"model\": model,\n",
    "                \"dataset\": dataset,\n",
    "                \"clusterer\": clusterer,\n",
    "                \"aggclust_dist_thresh\": None,\n",
    "                \"zscore\": False,\n",
    "                \"dim_reducer\": \"None\",\n",
    "                \"dim_reducer_man\": \"None\",\n",
    "            }\n",
    "            sdf = select_rows(runs_df, filter, allow_missing=True)\n",
    "            filter2 = dict(DEFAULT_PARAMS[\"all\"], **DEFAULT_PARAMS[clusterer])\n",
    "            filter2 = {k: v for k, v in filter2.items() if k not in filter}\n",
    "            sdf = select_rows(sdf, filter2, allow_missing=False)\n",
    "            if len(sdf) < 1:\n",
    "                if dataset == \"imagenet\" and clusterer in [\n",
    "                    \"AffinityPropagation\",\n",
    "                    \"SpectralClustering\",\n",
    "                ]:\n",
    "                    continue\n",
    "                if clusterer in [\"SpectralClustering\"]:\n",
    "                    continue\n",
    "                print(\"No data for\", filter)\n",
    "                cmds.append(filter2command(filter, filter2))\n",
    "                continue\n",
    "            if len(sdf) > 1:\n",
    "                if sum(sdf[\"AMI\"] != sdf.iloc[0][\"AMI\"]) > 0:\n",
    "                    print()\n",
    "                    print(\"More than one result with AMIs:\", list(sdf[\"AMI\"]))\n",
    "                    print(f\"for search {filter} and {filter2}\")\n",
    "                    dif_cols = find_differing_columns(sdf, config_keys)\n",
    "                    print(f\"columns which differ: {dif_cols}\")\n",
    "                    print()\n",
    "            data_base[i_clusterer, i_model, i_dataset, 0] = np.median(sdf[\"AMI\"])\n",
    "            # z-score\n",
    "            filter = {\n",
    "                \"model\": model,\n",
    "                \"dataset\": dataset,\n",
    "                \"clusterer\": clusterer,\n",
    "                \"aggclust_dist_thresh\": None,\n",
    "                \"zscore\": True,\n",
    "                \"dim_reducer\": \"None\",\n",
    "                \"dim_reducer_man\": \"None\",\n",
    "            }\n",
    "            sdf = select_rows(runs_df, filter, allow_missing=False)\n",
    "            filter2 = dict(DEFAULT_PARAMS[\"all\"], **DEFAULT_PARAMS[clusterer])\n",
    "            filter2 = {k: v for k, v in filter2.items() if k not in filter}\n",
    "            sdf = select_rows(sdf, filter2, allow_missing=False)\n",
    "            if len(sdf) < 1:\n",
    "                if dataset == \"imagenet\" and clusterer in [\n",
    "                    \"AffinityPropagation\",\n",
    "                    \"SpectralClustering\",\n",
    "                ]:\n",
    "                    continue\n",
    "                if clusterer in [\"SpectralClustering\"]:\n",
    "                    continue\n",
    "                print(\"No data for\", filter)\n",
    "                cmds.append(filter2command(filter, filter2))\n",
    "                continue\n",
    "            if len(sdf) > 1:\n",
    "                if sum(sdf[\"AMI\"] != sdf.iloc[0][\"AMI\"]) > 0:\n",
    "                    print()\n",
    "                    print(\"More than one result with AMIs:\", list(sdf[\"AMI\"]))\n",
    "                    print(f\"for search {filter} and {filter2}\")\n",
    "                    dif_cols = find_differing_columns(sdf, config_keys)\n",
    "                    print(f\"columns which differ: {dif_cols}\")\n",
    "                    print()\n",
    "            data_base[i_clusterer, i_model, i_dataset, 1] = np.median(sdf[\"AMI\"])\n",
    "\n",
    "for cmd in cmds:\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_iZBvJuY0Tlq"
   },
   "outputs": [],
   "source": [
    "np.nanmax(data_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DHvpqopo3Iz5"
   },
   "outputs": [],
   "source": [
    "np.nanmax(data_umap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5f6OBSOKEJHP"
   },
   "outputs": [],
   "source": [
    "np.nanmax(data_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0RqhXFtdzPF9"
   },
   "outputs": [],
   "source": [
    "data_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nrc_iZecEZEG"
   },
   "outputs": [],
   "source": [
    "# Weight imagenet twice as much as imagenette and imagewoof\n",
    "# weights_val = np.ones((len(VALIDATION_DATASETS), ), dtype=int)\n",
    "weights_val = np.array([2 if d == \"imagenet\" else 1 for d in VALIDATION_DATASETS])\n",
    "\n",
    "cmap = categorical_cmap(4, len(VALIDATION_DATASETS))\n",
    "width = 0.2\n",
    "\n",
    "# data_pca[i_clusterer, i_model, i_dataset, i_value]\n",
    "best_pca = np.nanmax(data_pca, axis=-1)\n",
    "best_umap = np.nanmax(data_umap, axis=-1)\n",
    "\n",
    "for i_clusterer, clusterer in enumerate(CLUSTERERS):\n",
    "    if clusterer in [\"SpectralClustering\"]:  # , \"OPTICS\"]:\n",
    "        continue\n",
    "    for i_model, model in enumerate(models):\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        ax = plt.axes()\n",
    "        my_data = np.NaN * np.ones((len(VALIDATION_DATASETS), 4))\n",
    "        for i_dataset, dataset in enumerate(VALIDATION_DATASETS):\n",
    "            i = 0\n",
    "            plt.bar(\n",
    "                i_dataset + i * width,\n",
    "                data_base[i_clusterer, i_model, i_dataset, 0],\n",
    "                width=width,\n",
    "                label=f\"{dataset}: original embeddings\",\n",
    "                color=cmap(i * 3 + i_dataset),\n",
    "            )\n",
    "            my_data[i_dataset, i] = data_base[i_clusterer, i_model, i_dataset, 0]\n",
    "            i += 1\n",
    "            plt.bar(\n",
    "                i_dataset + i * width,\n",
    "                data_base[i_clusterer, i_model, i_dataset, 1],\n",
    "                width=width,\n",
    "                label=f\"{dataset}: zscored embeddings\",\n",
    "                color=cmap(i * 3 + i_dataset),\n",
    "            )\n",
    "            my_data[i_dataset, i] = data_base[i_clusterer, i_model, i_dataset, 1]\n",
    "            i += 1\n",
    "            plt.bar(\n",
    "                i_dataset + i * width,\n",
    "                best_pca[i_clusterer, i_model, i_dataset],\n",
    "                width=width,\n",
    "                label=f\"{dataset}: PCA (best)\",\n",
    "                color=cmap(i * 3 + i_dataset),\n",
    "            )\n",
    "            my_data[i_dataset, i] = best_pca[i_clusterer, i_model, i_dataset]\n",
    "            i += 1\n",
    "            plt.bar(\n",
    "                i_dataset + i * width,\n",
    "                best_umap[i_clusterer, i_model, i_dataset],\n",
    "                width=width,\n",
    "                label=f\"{dataset}: UMAP (best)\",\n",
    "                color=cmap(i * 3 + i_dataset),\n",
    "            )\n",
    "            my_data[i_dataset, i] = best_umap[i_clusterer, i_model, i_dataset]\n",
    "\n",
    "        my_weights = weights_val\n",
    "        if clusterer in [\"AffinityPropagation\", \"SpectralClustering\"]:\n",
    "            # Disregard imagenet results as it has too many samples to run\n",
    "            my_data = my_data[1:]\n",
    "            my_weights = my_weights[1:]\n",
    "\n",
    "        my_data = np.average(my_data, axis=0, weights=my_weights)\n",
    "        hs = []\n",
    "        labels = [\n",
    "            \"original embeddings\",\n",
    "            \"zscored-embeddings\",\n",
    "            \"PCA (best)\",\n",
    "            \"UMAP (best)\",\n",
    "        ]\n",
    "        i = 0\n",
    "        hs.append(\n",
    "            plt.bar(\n",
    "                1 + i_dataset + i * width,\n",
    "                my_data[i],\n",
    "                width=width,\n",
    "                label=labels[i],\n",
    "                color=cmap(i * 3),\n",
    "            )\n",
    "        )\n",
    "        i += 1\n",
    "        hs.append(\n",
    "            plt.bar(\n",
    "                1 + i_dataset + i * width,\n",
    "                my_data[i],\n",
    "                width=width,\n",
    "                label=labels[i],\n",
    "                color=cmap(i * 3),\n",
    "            )\n",
    "        )\n",
    "        i += 1\n",
    "        hs.append(\n",
    "            plt.bar(\n",
    "                1 + i_dataset + i * width,\n",
    "                my_data[i],\n",
    "                width=width,\n",
    "                label=labels[i],\n",
    "                color=cmap(i * 3),\n",
    "            )\n",
    "        )\n",
    "        i += 1\n",
    "        hs.append(\n",
    "            plt.bar(\n",
    "                1 + i_dataset + i * width,\n",
    "                my_data[i],\n",
    "                width=width,\n",
    "                label=labels[i],\n",
    "                color=cmap(i * 3),\n",
    "            )\n",
    "        )\n",
    "        ax.set_xticks(\n",
    "            np.arange(len(VALIDATION_DATASETS) + 1) + width * i / 2,\n",
    "            VALIDATION_DATASETS + [\"mean\"],\n",
    "        )\n",
    "        plt.ylabel(\"AMI\")\n",
    "        try:\n",
    "            best_option = labels[np.nanargmax(my_data)]\n",
    "        except Exception:\n",
    "            best_option = \"n/a\"\n",
    "        plt.title(f\"{clusterer}, {model} [{best_option}]\")\n",
    "        plt.ylim([-0.05, 1.05])\n",
    "        # plt.legend(handles=hs)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kBFncKpIUAfv"
   },
   "outputs": [],
   "source": [
    "dim_choices_rows = []\n",
    "eps = 1e-3\n",
    "# Weight imagenet twice as much as imagenette and imagewoof\n",
    "# weights_val = np.ones((len(VALIDATION_DATASETS), ), dtype=int)\n",
    "weights_val = np.array([2 if d == \"imagenet\" else 1 for d in VALIDATION_DATASETS])\n",
    "\n",
    "# cmap = categorical_cmap(len(models), len(CLUSTERERS))\n",
    "cmap = categorical_cmap(4, len(VALIDATION_DATASETS))\n",
    "\n",
    "axis_values = [2, 5, 10, 20, 50, 100, 200, 500]\n",
    "# axis_values = [2, 5, 10, 20, 50, 100, 200]\n",
    "\n",
    "i = 0\n",
    "for i_clusterer, clusterer in enumerate(CLUSTERERS):\n",
    "    if clusterer in [\"SpectralClustering\"]:\n",
    "        continue\n",
    "    for i_model, model in enumerate(models):\n",
    "        i += 1\n",
    "        my_data_p = data_pca[i_clusterer, i_model]  # [:, :-1]\n",
    "        my_data_u = data_umap[i_clusterer, i_model]  # [:, :-1]\n",
    "        my_data_pvar = data_pca_var[i_clusterer, i_model]\n",
    "        my_data_base = data_base[i_clusterer, i_model]\n",
    "        plt.figure()\n",
    "        # indiv\n",
    "        for i_dataset, dataset in enumerate(VALIDATION_DATASETS):\n",
    "            if dataset == \"imagenet\" and clusterer in [\n",
    "                \"AffinityPropagation\",\n",
    "                \"SpectralClustering\",\n",
    "            ]:\n",
    "                # No imagenet results as it has too many samples\n",
    "                continue\n",
    "            ls = DATASET2LS.get(dataset) + \"o\"\n",
    "            plt.plot(\n",
    "                axis_values,\n",
    "                my_data_p[i_dataset],\n",
    "                ls,\n",
    "                markersize=5,\n",
    "                color=cmap(2 * 3 + 2),\n",
    "            )\n",
    "            plt.plot(\n",
    "                axis_values,\n",
    "                my_data_u[i_dataset],\n",
    "                ls,\n",
    "                markersize=5,\n",
    "                color=cmap(3 * 3 + 2),\n",
    "            )\n",
    "        # mean\n",
    "        if clusterer in [\"AffinityPropagation\", \"SpectralClustering\"]:\n",
    "            # No imagenet results as it has too many samples\n",
    "            my_data_p = my_data_p[1:]\n",
    "            my_data_u = my_data_u[1:]\n",
    "            my_data_pvar = my_data_pvar[1:]\n",
    "            my_data_base = my_data_base[1:]\n",
    "            my_weights = weights_val[1:]\n",
    "        else:\n",
    "            my_weights = weights_val\n",
    "        mu_data_p = np.average(my_data_p, axis=0, weights=my_weights)\n",
    "        mu_data_u = np.average(my_data_u, axis=0, weights=my_weights)\n",
    "        mu_data_pvar = np.average(my_data_pvar, axis=0, weights=my_weights)\n",
    "        my_data_base = np.average(my_data_base, axis=0, weights=my_weights)\n",
    "        plt.plot(axis_values, mu_data_p, \"o-\", markersize=5, color=cmap(2 * 3))\n",
    "        plt.plot(axis_values, mu_data_u, \"o-\", markersize=5, color=cmap(3 * 3))\n",
    "        if (\n",
    "            sum(~np.isnan(mu_data_p)) == 0\n",
    "            or sum(~np.isnan(mu_data_u)) == 0\n",
    "            or sum(~np.isnan(mu_data_pvar)) == 0\n",
    "        ):\n",
    "            plt.title(f\"{clusterer}, {model} [MISSING DATA]\")\n",
    "            plt.xscale(\"log\")\n",
    "            plt.xlabel(\"Num dimensions\")\n",
    "            plt.ylabel(\"AMI\")\n",
    "            plt.ylim([-0.05, 1.05])\n",
    "            plt.show()\n",
    "            continue\n",
    "        best_pca_i = np.nanargmax(mu_data_p)\n",
    "        best_umap_i = np.nanargmax(mu_data_u)\n",
    "        best_pvar_i = np.nanargmax(mu_data_pvar)\n",
    "        if mu_data_p[best_pca_i] > mu_data_u[best_umap_i]:\n",
    "            best_reducer = \"PCA\"\n",
    "            best_d = axis_values[best_pca_i]\n",
    "            best_ami_plot = mu_data_p[best_pca_i]\n",
    "            plt.plot(best_d, best_ami_plot, \"kx\")\n",
    "            row = {\n",
    "                \"clusterer\": clusterer,\n",
    "                \"model\": model,\n",
    "                \"reducer\": \"PCA\",\n",
    "                \"dim\": best_d,\n",
    "            }\n",
    "        else:\n",
    "            best_reducer = \"UMAP\"\n",
    "            best_d = axis_values[best_umap_i]\n",
    "            best_ami_plot = mu_data_u[best_umap_i]\n",
    "            plt.plot(best_d, best_ami_plot, \"kx\")\n",
    "            row = {\n",
    "                \"clusterer\": clusterer,\n",
    "                \"model\": model,\n",
    "                \"reducer\": \"UMAP\",\n",
    "                \"dim\": best_d,\n",
    "            }\n",
    "        best_ami = np.nanmax(\n",
    "            [\n",
    "                max(my_data_base),\n",
    "                max(mu_data_p),\n",
    "                max(mu_data_u),\n",
    "                max(mu_data_pvar),\n",
    "            ]\n",
    "        )\n",
    "        if best_ami <= best_ami_plot:\n",
    "            extra_str = \"best\"\n",
    "        else:\n",
    "            extra_str = f\"< {best_ami:.3f} from\"\n",
    "            if best_ami == my_data_base[0]:\n",
    "                extra_str += \" full\"\n",
    "                if best_ami >= best_ami_plot + eps:\n",
    "                    row = {\"clusterer\": clusterer, \"model\": model, \"reducer\": \"OG\"}\n",
    "                    extra_str += \"*\"\n",
    "            if best_ami == my_data_base[1]:\n",
    "                extra_str += \" fullzscore\"\n",
    "                if best_ami >= best_ami_plot + eps:\n",
    "                    row = {\n",
    "                        \"clusterer\": clusterer,\n",
    "                        \"model\": model,\n",
    "                        \"reducer\": \"zscore-only\",\n",
    "                    }\n",
    "                    extra_str += \"*\"\n",
    "            if best_ami == max(mu_data_pvar):\n",
    "                extra_str += f\" PCA var={pca_var_values[best_pvar_i]}\"\n",
    "                if best_ami >= best_ami_plot + eps:\n",
    "                    row = {\n",
    "                        \"clusterer\": clusterer,\n",
    "                        \"model\": model,\n",
    "                        \"reducer\": \"PCA\",\n",
    "                        \"dim\": pca_var_values[best_pvar_i],\n",
    "                    }\n",
    "                    extra_str += \"*\"\n",
    "        plt.title(\n",
    "            f\"{clusterer}, {model}\"\n",
    "            f\"  [{best_reducer} {best_d}: AMI={best_ami_plot:.3f} ({extra_str})]\"\n",
    "        )\n",
    "        plt.xscale(\"log\")\n",
    "        plt.xlabel(\"Num dimensions\")\n",
    "        plt.ylabel(\"AMI\")\n",
    "        plt.ylim([-0.05, 1.05])\n",
    "        plt.show()\n",
    "        dim_choices_rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ANCetb1cMRDE"
   },
   "outputs": [],
   "source": [
    "df_dim_choices = pd.DataFrame.from_records(dim_choices_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OwtO1oa6Mdgf"
   },
   "outputs": [],
   "source": [
    "df_dim_choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y9aSjzsxOtm1"
   },
   "outputs": [],
   "source": [
    "# cmap = categorical_cmap(len(models), len(CLUSTERERS))\n",
    "cmap = categorical_cmap(4, len(VALIDATION_DATASETS))\n",
    "\n",
    "i = 0\n",
    "for i_clusterer, clusterer in enumerate(CLUSTERERS):\n",
    "    if clusterer in [\"SpectralClustering\"]:\n",
    "        continue\n",
    "    for i_model, model in enumerate(models):\n",
    "        i += 1\n",
    "        my_data_pvar = data_pca_var[i_clusterer, i_model]\n",
    "        plt.figure()\n",
    "        # indiv\n",
    "        for i_dataset, dataset in enumerate(VALIDATION_DATASETS):\n",
    "            if dataset == \"imagenet\" and clusterer in [\n",
    "                \"AffinityPropagation\",\n",
    "                \"SpectralClustering\",\n",
    "            ]:\n",
    "                # No imagenet results as it has too many samples\n",
    "                continue\n",
    "            ls = DATASET2LS.get(dataset) + \"o\"\n",
    "            plt.plot(\n",
    "                pca_var_values,\n",
    "                my_data_pvar[i_dataset],\n",
    "                ls,\n",
    "                markersize=5,\n",
    "                color=cmap(2 * 3 + 2),\n",
    "            )\n",
    "        # mean\n",
    "        if clusterer in [\"AffinityPropagation\", \"SpectralClustering\"]:\n",
    "            # No imagenet results as it has too many samples\n",
    "            my_data_pvar = my_data_pvar[1:]\n",
    "            my_weights = weights_val[1:]\n",
    "        else:\n",
    "            my_weights = weights_val\n",
    "        mu_data_pvar = np.average(my_data_pvar, axis=0, weights=my_weights)\n",
    "        plt.plot(pca_var_values, mu_data_pvar, \"-o\", markersize=5, color=cmap(2 * 3))\n",
    "        plt.title(f\"{clusterer}, {model}\")\n",
    "        # plt.xscale(\"log\")\n",
    "        plt.xlabel(\"Variance kept\")\n",
    "        plt.ylabel(\"AMI\")\n",
    "        plt.ylim([-0.05, 1.05])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAgJs3H-uqiJ"
   },
   "outputs": [],
   "source": [
    "models = RESNET50_MODELS + VITB16_MODELS\n",
    "BEST_PARAMS = {\n",
    "    clusterer: {model: copy.deepcopy(DEFAULT_PARAMS[clusterer]) for model in models}\n",
    "    for clusterer in ALL_CLUSTERERS\n",
    "}\n",
    "\n",
    "# KMeans\n",
    "# Use UMAP (num dims unimportant; we select 50d for consistency) for every encoder except\n",
    "# - clip_RN50 : a little better to use PCA with 500d than UMAP. UMAP beats PCA if you\n",
    "#   reduce the PCA dims below 500.\n",
    "# - clip_vitb16 : same behaviour as clip_RN50\n",
    "# - timm_vit_base_patch16_224.mae : best is PCA 0.85 variance explained. Need at least\n",
    "#   200 PCA dims, and PCA perf beats UMAP throughout\n",
    "\n",
    "for model in RESNET50_MODELS + VITB16_MODELS:\n",
    "    if model.startswith(\"clip\") or model == \"timm_vit_base_patch16_224.mae\":\n",
    "        continue\n",
    "    BEST_PARAMS[\"KMeans\"][model].update(\n",
    "        {\"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50}\n",
    "    )\n",
    "\n",
    "BEST_PARAMS[\"KMeans\"][\"clip_RN50\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"ndim_reduced\": 500, \"zscore\": True, \"pca_variance\": None}\n",
    ")\n",
    "BEST_PARAMS[\"KMeans\"][\"clip_vitb16\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"ndim_reduced\": 500, \"zscore\": True, \"pca_variance\": None}\n",
    ")\n",
    "BEST_PARAMS[\"KMeans\"][\"timm_vit_base_patch16_224.mae\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"pca_variance\": 0.85, \"zscore\": True, \"ndim_reduced\": None}\n",
    ")\n",
    "\n",
    "# AffinityPropagation\n",
    "# Use PCA with 10 dims for every encoder except\n",
    "# - resnet50 (supervised) : original embeddings, no reduction (AMI=0.62);\n",
    "#   perf gets worse if they are whitened (AMI=0.55) and although the perf increases\n",
    "#   as num dims are reduced it doesn't quite recover. PCA perf peaks at 10-20 dim (AMI=0.57).\n",
    "# - dino_resnet50 : does marginally better at UMAP 50 (AMI=0.52495) than PCA 10 (AMI=0.5044)\n",
    "# - timm_vit_base_patch16_224.mae : PCA 0.95 variance explained (AMI=0.303).\n",
    "#   Definite improvement from 10 to 20 dims, but not much improvement above that.\n",
    "\n",
    "for model in models:\n",
    "    if model in [\"resnet50\", \"dino_resnet50\", \"timm_vit_base_patch16_224.mae\"]:\n",
    "        continue\n",
    "    BEST_PARAMS[\"AffinityPropagation\"][model].update(\n",
    "        {\n",
    "            \"dim_reducer\": \"PCA\",\n",
    "            \"ndim_reduced\": 10,\n",
    "            \"zscore\": True,\n",
    "            \"pca_variance\": None,\n",
    "            \"dim_reducer_man\": \"None\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "BEST_PARAMS[\"AffinityPropagation\"][\"resnet50\"].update(\n",
    "    {\"dim_reducer\": \"None\", \"dim_reducer_man\": \"None\", \"zscore\": False}\n",
    ")\n",
    "BEST_PARAMS[\"AffinityPropagation\"][\"dino_resnet50\"].update(\n",
    "    {\n",
    "        \"dim_reducer\": \"PCA\",\n",
    "        \"pca_variance\": 0.95,\n",
    "        \"zscore\": True,\n",
    "        \"ndim_reduced\": None,\n",
    "        \"dim_reducer_man\": \"None\",\n",
    "    }\n",
    ")\n",
    "BEST_PARAMS[\"AffinityPropagation\"][\"timm_vit_base_patch16_224.mae\"].update(\n",
    "    {\n",
    "        \"dim_reducer\": \"PCA\",\n",
    "        \"pca_variance\": 0.95,\n",
    "        \"zscore\": True,\n",
    "        \"ndim_reduced\": None,\n",
    "        \"dim_reducer_man\": \"None\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# AgglomerativeClustering\n",
    "# Use UMAP (num dims unimportant; we select 50d for consistency) for every encoder except\n",
    "# - timm_vit_base_patch16_224.mae : PCA 0.98 variance explained (i.e. nearly all\n",
    "#   dimensions kept), which is not noticably better than using 500 dim PCA but there is\n",
    "#   an increase compared to using less than 500d.\n",
    "\n",
    "for model in models:\n",
    "    if model == \"timm_vit_base_patch16_224.mae\":\n",
    "        continue\n",
    "    BEST_PARAMS[\"AgglomerativeClustering\"][model].update(\n",
    "        {\"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50, \"dim_reducer\": \"None\"}\n",
    "    )\n",
    "\n",
    "BEST_PARAMS[\"AgglomerativeClustering\"][\"timm_vit_base_patch16_224.mae\"].update(\n",
    "    {\n",
    "        \"dim_reducer\": \"PCA\",\n",
    "        \"pca_variance\": 0.98,\n",
    "        \"zscore\": True,\n",
    "        \"ndim_reduced\": None,\n",
    "        \"dim_reducer_man\": \"None\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# HDBSCAN\n",
    "# Use UMAP for every encoder except\n",
    "# - timm_vit_base_patch16_224.mae : PCA 0.95 variance explained (AMI=0.085) which is\n",
    "#   not noticably better than PCA with 50 dim\n",
    "\n",
    "for model in models:\n",
    "    if model in [\"timm_vit_base_patch16_224.mae\"]:\n",
    "        continue\n",
    "    BEST_PARAMS[\"HDBSCAN\"][model].update(\n",
    "        {\"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50, \"dim_reducer\": \"None\"}\n",
    "    )\n",
    "\n",
    "BEST_PARAMS[\"HDBSCAN\"][\"timm_vit_base_patch16_224.mae\"].update(\n",
    "    {\n",
    "        \"dim_reducer\": \"PCA\",\n",
    "        \"pca_variance\": 0.95,\n",
    "        \"zscore\": True,\n",
    "        \"ndim_reduced\": None,\n",
    "        \"dim_reducer_man\": \"None\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# OPTICS\n",
    "# Use UMAP for every encoder, no exceptions necessary\n",
    "for model in models:\n",
    "    BEST_PARAMS[\"OPTICS\"][model].update(\n",
    "        {\"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50, \"dim_reducer\": \"None\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2nA5YxFqFo7x",
    "outputId": "0914fd9c-ca24-4e60-9f90-ad329f877d9d"
   },
   "outputs": [],
   "source": [
    "BEST_PARAMS_v1 = copy.deepcopy(BEST_PARAMS)\n",
    "BEST_PARAMS_v2 = BEST_PARAMS\n",
    "\n",
    "print(\"Updating dim choices for new method\")\n",
    "# Updated dim choices\n",
    "# (changed to this when we swapped to using weighted average instead of straight\n",
    "# average between Imagenet-1k, Imagenette, Imagewoof)\n",
    "\n",
    "# Changed KMeans clip_RN50 from PCA 500 to UMAP 50, so it uses fewer dimensions\n",
    "# (probably more stable than using 500-d which is what PCA needs to marginally beat UMAP)\n",
    "BEST_PARAMS_v2[\"KMeans\"][\"clip_RN50\"].update(\n",
    "    {\"dim_reducer\": None, \"ndim_reduced\": None, \"zscore\": False, \"pca_variance\": None}\n",
    ")\n",
    "BEST_PARAMS_v2[\"KMeans\"][\"clip_RN50\"].update(\n",
    "    {\"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50}\n",
    ")\n",
    "# Changed KMeans MAE from PCA 85% to PCA 200\n",
    "# (since we see perf above plateaus at 200-d, there is no point going above that)\n",
    "BEST_PARAMS_v2[\"KMeans\"][\"timm_vit_base_patch16_224.mae\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"zscore\": True, \"ndim_reduced\": 200, \"pca_variance\": None}\n",
    ")\n",
    "# Changed KMeans clip_vitb16 from PCA 500 to PCA 75%\n",
    "# (gives a notably better train set AMI measurement above)\n",
    "BEST_PARAMS_v2[\"KMeans\"][\"clip_vitb16\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"zscore\": True, \"ndim_reduced\": None, \"pca_variance\": 0.75}\n",
    ")\n",
    "\n",
    "# Changed AffinityPropagation dino_resnet50 from PCA 95% to PCA 10\n",
    "# (performance is basically equal, so no point using higher-dim space;\n",
    "# could have done UMAP 50 instead with basically equal train AMI to PCA 10,\n",
    "# but didn't for consistency with other models)\n",
    "BEST_PARAMS_v2[\"AffinityPropagation\"][\"dino_resnet50\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"zscore\": True, \"ndim_reduced\": 10, \"pca_variance\": None}\n",
    ")\n",
    "# Changed AffinityPropagation MAE from PCA 95% to PCA 100\n",
    "BEST_PARAMS_v2[\"AffinityPropagation\"][\"timm_vit_base_patch16_224.mae\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"zscore\": True, \"ndim_reduced\": 100, \"pca_variance\": None}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKQJ1xoMHDu3"
   },
   "source": [
    "## Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53fFD0J2c6e7"
   },
   "source": [
    "### AgglomerativeClustering metric and linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2w-hy7aHQ-AP"
   },
   "outputs": [],
   "source": [
    "models = RESNET50_MODELS + VITB16_MODELS\n",
    "cmap = categorical_cmap(len(models), len(VALIDATION_DATASETS))\n",
    "clusterer = \"AgglomerativeClustering\"\n",
    "methods = [\"ward\", \"complete\", \"average\", \"single\"]\n",
    "metrics = [\"euclidean\", \"l1\", \"chebyshev\", \"cosine\"]  # \"arccos\"\n",
    "\n",
    "data = np.NaN * np.ones(\n",
    "    (len(models), len(VALIDATION_DATASETS), len(methods), len(metrics))\n",
    ")\n",
    "cmds = []\n",
    "for i_model, model in enumerate(models):\n",
    "    for i_dataset, dataset in enumerate(VALIDATION_DATASETS):\n",
    "        for i_method, method in enumerate(methods):\n",
    "            for i_metric, metric in enumerate(metrics):\n",
    "                filter = {\n",
    "                    \"model\": model,\n",
    "                    \"dataset\": dataset,\n",
    "                    \"clusterer\": clusterer,\n",
    "                    # \"dim_reducer_man\": \"UMAP\",\n",
    "                    # \"ndim_reduced_man\": 50,\n",
    "                    \"distance_metric\": metric,\n",
    "                    \"aggclust_linkage\": method,\n",
    "                    \"aggclust_dist_thresh\": None,\n",
    "                }\n",
    "                sdf = select_rows(runs_df, filter, allow_missing=False)\n",
    "                filter2 = dict(DEFAULT_PARAMS[\"all\"], **BEST_PARAMS[clusterer][model])\n",
    "                filter2 = {k: v for k, v in filter2.items() if k not in filter}\n",
    "                sdf = select_rows(sdf, filter2, allow_missing=False)\n",
    "                if len(sdf) < 1:\n",
    "                    if method == \"ward\" and metric not in [\"euclidean\", \"arccos\"]:\n",
    "                        # expected not to exist\n",
    "                        continue\n",
    "                    print(\"No data for\", filter)\n",
    "                    cmds.append(filter2command(filter, filter2))\n",
    "                    continue\n",
    "                if len(sdf) > 1:\n",
    "                    if sum(sdf[\"AMI\"] != sdf.iloc[0][\"AMI\"]) > 0:\n",
    "                        print()\n",
    "                        print(\"More than one result with AMIs:\", list(sdf[\"AMI\"]))\n",
    "                        print(f\"for search {filter}\\nand {filter2}\")\n",
    "                        dif_cols = find_differing_columns(sdf, config_keys)\n",
    "                        print(f\"columns which differ: {dif_cols}\")\n",
    "                        if dif_cols:\n",
    "                            for col in dif_cols:\n",
    "                                print(f\"  {col}: {list(sdf[col])}\")\n",
    "                data[i_model, i_dataset, i_method, i_metric] = np.median(sdf[\"AMI\"])\n",
    "\n",
    "if len(cmds) > 0:\n",
    "    print()\n",
    "for cmd in cmds:\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MvNQovQqm-0P"
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eL4k6iWouvBJ"
   },
   "outputs": [],
   "source": [
    "np.sum(np.isnan(data)) / data.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eq1ZnGtRup65"
   },
   "outputs": [],
   "source": [
    "cmap = categorical_cmap(len(metrics), len(methods))\n",
    "\n",
    "width = 1 / (len(methods) * len(metrics) + 2)\n",
    "YLIM = [-0.05, 1.05]\n",
    "for i_model, model in enumerate(models):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = plt.axes()\n",
    "    # for i_dataset, dataset in enumerate(VALIDATION_DATASETS):\n",
    "    i = -1\n",
    "    for i_metric, metric in enumerate(metrics):\n",
    "        for i_method, method in enumerate(methods):\n",
    "            i += 1\n",
    "            plt.bar(\n",
    "                np.arange(len(VALIDATION_DATASETS)) + i * width,\n",
    "                data[i_model, :, i_method, i_metric],\n",
    "                width=width,\n",
    "                label=f\"{metric}: {method}\",\n",
    "                color=cmap(i),\n",
    "            )\n",
    "    plt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "    ax.set_xticks(\n",
    "        np.arange(len(VALIDATION_DATASETS)) + width * (i + 1) / 2, VALIDATION_DATASETS\n",
    "    )\n",
    "    plt.ylim(YLIM)\n",
    "    plt.title(model)\n",
    "    plt.ylabel(\"AMI\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_MjtzkG1MX4"
   },
   "outputs": [],
   "source": [
    "agglink_choices_rows = []\n",
    "# weights_val = np.ones((len(VALIDATION_DATASETS), ), dtype=int)\n",
    "weights_val = np.array([2 if d == \"imagenet\" else 1 for d in VALIDATION_DATASETS])\n",
    "\n",
    "cmap = categorical_cmap(len(metrics), len(methods))\n",
    "\n",
    "avg_ami = np.average(data, axis=1, weights=weights_val)\n",
    "\n",
    "width = 1 / (len(methods) + 2)\n",
    "YLIM = [-0.05, 1.05]\n",
    "for i_model, model in enumerate(models):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = plt.axes()\n",
    "    # for i_dataset, dataset in enumerate(VALIDATION_DATASETS):\n",
    "    i = -1\n",
    "    min_ami = 1\n",
    "    max_ami = 0\n",
    "    for i_metric, metric in enumerate(metrics):\n",
    "        for i_method, method in enumerate(methods):\n",
    "            i += 1\n",
    "            my_val = avg_ami[i_model, i_method, i_metric]\n",
    "            if np.isnan(my_val):\n",
    "                continue\n",
    "            min_ami = np.nanmin([min_ami, my_val])\n",
    "            max_ami = np.nanmax([max_ami, my_val])\n",
    "            plt.bar(\n",
    "                i_metric + i_method * width,\n",
    "                my_val,\n",
    "                width=width,\n",
    "                label=f\"{metric}: {method}\",\n",
    "                color=cmap(i),\n",
    "            )\n",
    "    # Exclude cosine distance because it doesn't make sense with UMAP generally\n",
    "    # (the origin is ill-defined)\n",
    "    best_method_idx, best_metric_idx = np.unravel_index(\n",
    "        np.nanargmax(avg_ami[i_model, :, :3]),\n",
    "        avg_ami[i_model, :, :3].shape,\n",
    "    )\n",
    "    best_ami = avg_ami[i_model, best_method_idx, best_metric_idx]\n",
    "    ax.set_xticks(np.arange(len(metrics)) + width * (i_method + 1) / 2, metrics)\n",
    "    YLIM = np.array([min_ami, max_ami])\n",
    "    YLIM += np.array([-1, 1]) * 0.05 * (YLIM[1] - YLIM[0])\n",
    "    plt.ylim(YLIM)\n",
    "    plt.title(\n",
    "        f\"{model} : {metrics[best_metric_idx]} {methods[best_method_idx]}\"\n",
    "        f\"  (AMI={best_ami:.3f})\"\n",
    "        f\"  ... {np.sort(avg_ami[i_model][~np.isnan(avg_ami[i_model])], axis=None)[-1:-5:-1]}\"\n",
    "    )\n",
    "    plt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "    plt.ylabel(\"AMI\")\n",
    "    plt.show()\n",
    "    row = {\n",
    "        \"model\": model,\n",
    "        \"distance_metric\": metrics[best_metric_idx],\n",
    "        \"aggclust_linkage\": methods[best_method_idx],\n",
    "        \"AMI\": best_ami,\n",
    "    }\n",
    "    agglink_choices_rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tzPoNNF9UB_a"
   },
   "outputs": [],
   "source": [
    "avg_ami[np.array(models) == \"vicreg_resnet50\", np.array(methods) == \"average\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AAZv_uI5Ut3T"
   },
   "outputs": [],
   "source": [
    "avg_ami[np.array(models) == \"vitb16\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j0EVjGW3EEaN"
   },
   "outputs": [],
   "source": [
    "agglink_choices_df = pd.DataFrame.from_dict(agglink_choices_rows)\n",
    "agglink_choices_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mthyE99vWAHn"
   },
   "outputs": [],
   "source": [
    "print(agglink_choices_df.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hg5yuYyOWEEM"
   },
   "source": [
    "With equal weighting between Imagenet, Imagenette, Imagewoof\n",
    "\n",
    "|    | model                         | distance_metric   | aggclust_linkage   |\n",
    "|---:|:------------------------------|:------------------|:-------------------|\n",
    "|  0 | resnet50                      | euclidean         | ward               |\n",
    "|  1 | mocov3_resnet50               | euclidean         | ward               |\n",
    "|  2 | vicreg_resnet50               | euclidean         | ward               |\n",
    "|  3 | dino_resnet50                 | euclidean         | average            |\n",
    "|  4 | clip_RN50                     | euclidean         | average            |\n",
    "|  5 | vitb16                        | euclidean         | ward               |\n",
    "|  6 | mocov3_vit_base               | chebyshev         | average            |\n",
    "|  7 | timm_vit_base_patch16_224.mae | euclidean         | ward               |\n",
    "|  8 | dino_vitb16                   | euclidean         | average            |\n",
    "|  9 | clip_vitb16                   | chebyshev         | average            |\n",
    "\n",
    "With 2:1:1 weighting\n",
    "\n",
    "|    | model                         | distance_metric   | aggclust_linkage   |      AMI |\n",
    "|---:|:------------------------------|:------------------|:-------------------|---------:|\n",
    "|  0 | resnet50                      | euclidean         | ward               | 0.880867 |\n",
    "|  1 | mocov3_resnet50               | euclidean         | ward               | 0.661994 |\n",
    "|  2 | vicreg_resnet50               | cosine            | average            | 0.623931 |\n",
    "|  3 | dino_resnet50                 | euclidean         | average            | 0.610076 |\n",
    "|  4 | clip_RN50                     | euclidean         | average            | 0.587426 |\n",
    "|  5 | vitb16                        | euclidean         | ward               | 0.937149 |\n",
    "|  6 | mocov3_vit_base               | chebyshev         | average            | 0.735425 |\n",
    "|  7 | timm_vit_base_patch16_224.mae | cosine            | average            | 0.300642 |\n",
    "|  8 | dino_vitb16                   | euclidean         | average            | 0.799998 |\n",
    "|  9 | clip_vitb16                   | chebyshev         | average            | 0.702895 |\n",
    "\n",
    "Excluding cosine\n",
    "\n",
    "|    | model                         | distance_metric   | aggclust_linkage   |      AMI |\n",
    "|---:|:------------------------------|:------------------|:-------------------|---------:|\n",
    "|  0 | resnet50                      | euclidean         | ward               | 0.880867 |\n",
    "|  1 | mocov3_resnet50               | euclidean         | ward               | 0.661994 |\n",
    "|  2 | vicreg_resnet50               | euclidean         | average            | 0.623852 |\n",
    "|  3 | dino_resnet50                 | euclidean         | average            | 0.610076 |\n",
    "|  4 | clip_RN50                     | euclidean         | average            | 0.587426 |\n",
    "|  5 | vitb16                        | euclidean         | ward               | 0.937149 |\n",
    "|  6 | mocov3_vit_base               | chebyshev         | average            | 0.735425 |\n",
    "|  7 | timm_vit_base_patch16_224.mae | euclidean         | ward               | 0.290479 |\n",
    "|  8 | dino_vitb16                   | euclidean         | average            | 0.799998 |\n",
    "|  9 | clip_vitb16                   | chebyshev         | average            | 0.702895 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cv8_GiZ5PfDF"
   },
   "outputs": [],
   "source": [
    "for model in [\n",
    "    \"resnet50\",\n",
    "    \"mocov3_resnet50\",\n",
    "    \"vicreg_resnet50\",\n",
    "    \"vitb16\",\n",
    "    \"timm_vit_base_patch16_224.mae\",\n",
    "]:\n",
    "    BEST_PARAMS_v1[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"euclidean\",\n",
    "            \"aggclust_linkage\": \"ward\",\n",
    "        }\n",
    "    )\n",
    "for model in [\"dino_resnet50\", \"clip_RN50\", \"dino_vitb16\"]:\n",
    "    BEST_PARAMS_v1[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"euclidean\",\n",
    "            \"aggclust_linkage\": \"average\",\n",
    "        }\n",
    "    )\n",
    "for model in [\"mocov3_vit_base\", \"clip_vitb16\"]:\n",
    "    BEST_PARAMS_v1[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"chebyshev\",\n",
    "            \"aggclust_linkage\": \"average\",\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4CouZvuSjPvc"
   },
   "outputs": [],
   "source": [
    "for model in [\"resnet50\", \"mocov3_resnet50\", \"vitb16\", \"timm_vit_base_patch16_224.mae\"]:\n",
    "    BEST_PARAMS_v2[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"euclidean\",\n",
    "            \"aggclust_linkage\": \"ward\",\n",
    "        }\n",
    "    )\n",
    "for model in [\"vicreg_resnet50\", \"dino_resnet50\", \"clip_RN50\", \"dino_vitb16\"]:\n",
    "    BEST_PARAMS_v2[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"euclidean\",\n",
    "            \"aggclust_linkage\": \"average\",\n",
    "        }\n",
    "    )\n",
    "for model in [\"mocov3_vit_base\", \"clip_vitb16\"]:\n",
    "    BEST_PARAMS_v2[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"chebyshev\",\n",
    "            \"aggclust_linkage\": \"average\",\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JgZudrwYujir"
   },
   "outputs": [],
   "source": [
    "for model in RESNET50_MODELS + VITB16_MODELS:\n",
    "    BEST_PARAMS_v2[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": agglink_choices_df[agglink_choices_df[\"model\"] == model][\n",
    "                \"distance_metric\"\n",
    "            ].item(),\n",
    "            \"aggclust_linkage\": agglink_choices_df[\n",
    "                agglink_choices_df[\"model\"] == model\n",
    "            ][\"aggclust_linkage\"].item(),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWdzGsvictp5"
   },
   "source": [
    "### AgglomerativeClustering distance threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-I6kIReSCGWr"
   },
   "outputs": [],
   "source": [
    "models = RESNET50_MODELS + VITB16_MODELS\n",
    "cmap = categorical_cmap(len(models), len(VALIDATION_DATASETS))\n",
    "clusterer = \"AgglomerativeClustering\"\n",
    "distance_thresholds = [\n",
    "    0.001,\n",
    "    0.002,\n",
    "    0.005,\n",
    "    0.01,\n",
    "    0.02,\n",
    "    0.05,\n",
    "    0.1,\n",
    "    0.2,\n",
    "    0.5,\n",
    "    1.0,\n",
    "    2.0,\n",
    "    5.0,\n",
    "    10.0,\n",
    "    20.0,\n",
    "    50.0,\n",
    "    100.0,\n",
    "    200.0,\n",
    "    500.0,\n",
    "    1000.0,\n",
    "    2000.0,\n",
    "    5000.0,\n",
    "]\n",
    "\n",
    "# Run with standardization to make things more comparable across datasets\n",
    "BEST_PARAMS = BEST_PARAMS_v2\n",
    "override_fields = {\n",
    "    \"zscore2\": \"average\",\n",
    "    \"ndim_correction\": True,\n",
    "}\n",
    "# No standardization (original configuration)\n",
    "# override_fields = {}\n",
    "\n",
    "data = np.NaN * np.ones(\n",
    "    (len(models), len(VALIDATION_DATASETS), len(distance_thresholds))\n",
    ")\n",
    "cmds = []\n",
    "for i_model, model in enumerate(models):\n",
    "    for i_dataset, dataset in enumerate(VALIDATION_DATASETS):\n",
    "        for i_thr, thr in enumerate(distance_thresholds):\n",
    "            filter = {\n",
    "                \"model\": model,\n",
    "                \"dataset\": dataset,\n",
    "                \"clusterer\": clusterer,\n",
    "                \"aggclust_dist_thresh\": thr,\n",
    "            }\n",
    "            filter.update(override_fields)\n",
    "            sdf = select_rows(runs_df, filter, allow_missing=False)\n",
    "            filter2 = dict(DEFAULT_PARAMS[\"all\"], **BEST_PARAMS[clusterer][model])\n",
    "            filter2 = {k: v for k, v in filter2.items() if k not in filter}\n",
    "            sdf = select_rows(sdf, filter2, allow_missing=False)\n",
    "            if len(sdf) < 1:\n",
    "                print(f\"No data for {filter} {filter2}\")\n",
    "                cmds.append(filter2command(filter, filter2))\n",
    "                continue\n",
    "            if len(sdf) > 1:\n",
    "                if sum(sdf[\"AMI\"] != sdf.iloc[0][\"AMI\"]) > 0:\n",
    "                    print()\n",
    "                    print(\"More than one result with AMIs:\", list(sdf[\"AMI\"]))\n",
    "                    print(f\"for search {filter}\\nand {filter2}\")\n",
    "                    dif_cols = find_differing_columns(sdf, config_keys)\n",
    "                    print(f\"columns which differ: {dif_cols}\")\n",
    "                    if dif_cols:\n",
    "                        for col in dif_cols:\n",
    "                            print(f\"  {col}: {list(sdf[col])}\")\n",
    "            data[i_model, i_dataset, i_thr] = np.median(sdf[\"AMI\"])\n",
    "\n",
    "if len(cmds) > 0:\n",
    "    print()\n",
    "for cmd in cmds:\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YWmU_WHSDKU1"
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ZZoRp5VEPOg"
   },
   "outputs": [],
   "source": [
    "aggthresh_choices_rows = []\n",
    "\n",
    "# weights_val = np.ones((len(VALIDATION_DATASETS), ), dtype=int)\n",
    "weights_val = np.array([2 if d == \"imagenet\" else 1 for d in VALIDATION_DATASETS])\n",
    "\n",
    "# Convert to relative performance compared with optimal threshold\n",
    "data_rel = data / np.nanmax(data, axis=-1, keepdims=True)\n",
    "mu_ami = np.average(data, axis=1, weights=weights_val)\n",
    "mu_amirel = np.average(data_rel, axis=1, weights=weights_val)\n",
    "\n",
    "# my_data, option = data, \"AMI\"\n",
    "my_data, option = data_rel, \"rel-AMI\"\n",
    "\n",
    "for i_model, model in enumerate(models):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    ax = plt.axes()\n",
    "    # indiv\n",
    "    # plt.plot(distance_thresholds, data[i_model].T, \":\", color=\"grey\")\n",
    "    for i_dataset, dataset in enumerate(VALIDATION_DATASETS):\n",
    "        ls = DATASET2LS.get(dataset)\n",
    "        ls = ls + (\"s\" if dataset == \"imagenet\" else \"o\")\n",
    "        c = \"grey\"  # \"green\" if dataset == \"imagenet\" else \"grey\"\n",
    "        ms = 5  # 7 if dataset == \"imagenet\" else 5\n",
    "        plt.plot(\n",
    "            distance_thresholds, my_data[i_model, i_dataset], ls, markersize=ms, color=c\n",
    "        )\n",
    "    plt.xlabel(\"Distance threshold\")\n",
    "    plt.ylabel(option)\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    if not np.all(np.isnan(data[i_model])):\n",
    "        plt.xscale(\"log\")\n",
    "    # mean\n",
    "    mu_data = np.average(my_data[i_model], axis=0, weights=weights_val)\n",
    "    if np.all(np.isnan(mu_data)):\n",
    "        print(f\"No data for {model}\")\n",
    "        continue\n",
    "    i_thr = np.nanargmax(mu_data)\n",
    "    best_thr = distance_thresholds[i_thr]\n",
    "    plt.plot(best_thr, mu_data[i_thr], \"x\")\n",
    "    plt.plot(distance_thresholds, mu_data, color=\"black\")\n",
    "    plt.title(f\"{model} (thr={best_thr}, {option}={mu_data[i_thr]})\")\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.show()\n",
    "    row = {\n",
    "        \"model\": model,\n",
    "        \"aggclust_dist_thresh\": best_thr,\n",
    "        \"rel-AMI\": mu_amirel[i_model, i_thr],\n",
    "        \"AMI\": mu_ami[i_model, i_thr],\n",
    "    }\n",
    "    aggthresh_choices_rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QYspXfe1ifWj"
   },
   "outputs": [],
   "source": [
    "aggthresh_choices_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8NkFFQJ6ilMY"
   },
   "outputs": [],
   "source": [
    "aggthresh_choices_df = pd.DataFrame.from_dict(aggthresh_choices_rows)\n",
    "aggthresh_choices_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f9pVd9BAaKRz"
   },
   "outputs": [],
   "source": [
    "print(aggthresh_choices_df.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqDn-JXmaMyt"
   },
   "source": [
    "Original version, with original metric/linkage selection (equal weighting), and equal weighting for threshold selection too.\n",
    "\n",
    "|    | model                         |   aggclust_dist_thresh |   rel-AMI |      AMI |\n",
    "|---:|:------------------------------|-----------------------:|----------:|---------:|\n",
    "|  0 | resnet50                      |                   20   |  0.945248 | 0.863248 |\n",
    "|  1 | mocov3_resnet50               |                   20   |  0.97133  | 0.702498 |\n",
    "|  2 | vicreg_resnet50               |                   20   |  0.989263 | 0.663419 |\n",
    "|  3 | dino_resnet50                 |                    1   |  0.988309 | 0.634068 |\n",
    "|  4 | clip_RN50                     |                    1   |  0.973346 | 0.605612 |\n",
    "|  5 | vitb16                        |                   20   |  0.957287 | 0.912559 |\n",
    "|  6 | mocov3_vit_base               |                    1   |  0.930347 | 0.721339 |\n",
    "|  7 | timm_vit_base_patch16_224.mae |                  200   |  0.824798 | 0.315614 |\n",
    "|  8 | dino_vitb16                   |                    2   |  0.937301 | 0.790191 |\n",
    "|  9 | clip_vitb16                   |                    0.5 |  0.954692 | 0.706133 |\n",
    "\n",
    "\n",
    "Use original metric/linkage, but change to 2:1:1 dataset weighting for threshold selection instead\n",
    "\n",
    "|    | model                         |   aggclust_dist_thresh |   rel-AMI |      AMI |\n",
    "|---:|:------------------------------|-----------------------:|----------:|---------:|\n",
    "|  0 | resnet50                      |                   10   |  0.937077 | 0.827762 |\n",
    "|  1 | mocov3_resnet50               |                   20   |  0.965668 | 0.648536 |\n",
    "|  2 | vicreg_resnet50               |                   20   |  0.985394 | 0.616488 |\n",
    "|  3 | dino_resnet50                 |                    1   |  0.983208 | 0.593935 |\n",
    "|  4 | clip_RN50                     |                    1   |  0.974061 | 0.57489  |\n",
    "|  5 | vitb16                        |                   10   |  0.942607 | 0.884239 |\n",
    "|  6 | mocov3_vit_base               |                    0.5 |  0.917765 | 0.671893 |\n",
    "|  7 | timm_vit_base_patch16_224.mae |                  200   |  0.868598 | 0.306396 |\n",
    "|  8 | dino_vitb16                   |                    2   |  0.908903 | 0.735008 |\n",
    "|  9 | clip_vitb16                   |                    0.5 |  0.954431 | 0.672023 |\n",
    "\n",
    "Change to selecting by rel-AMI instead (original metric/linkage, 2:1:1 weighting).\n",
    "\n",
    "|    | model                         |   aggclust_dist_thresh |   rel-AMI |      AMI |\n",
    "|---:|:------------------------------|-----------------------:|----------:|---------:|\n",
    "|  0 | resnet50                      |                   10   |  0.937077 | 0.827762 |\n",
    "|  1 | mocov3_resnet50               |                   20   |  0.965668 | 0.648536 |\n",
    "|  2 | vicreg_resnet50               |                   20   |  0.985394 | 0.616488 |\n",
    "|  3 | dino_resnet50                 |                    1   |  0.983208 | 0.593935 |\n",
    "|  4 | clip_RN50                     |                    1   |  0.974061 | 0.57489  |\n",
    "|  5 | vitb16                        |                   10   |  0.942607 | 0.884239 |\n",
    "|  6 | mocov3_vit_base               |                    0.5 |  0.917765 | 0.671893 |\n",
    "|  7 | timm_vit_base_patch16_224.mae |                  100   |  0.871321 | 0.294255 |\n",
    "|  8 | dino_vitb16                   |                    1   |  0.911006 | 0.725832 |\n",
    "|  9 | clip_vitb16                   |                    0.5 |  0.954431 | 0.672023 |\n",
    "\n",
    "Changing to new linkage (inc cosine) as well as using normalization of reduced embeddings.\n",
    "N.B. VICReg and MAE changed to cosine similarity when the linkage selection method was updated. (Work in progress.)\n",
    "\n",
    "|    | model                         |   aggclust_dist_thresh |   rel-AMI |      AMI |\n",
    "|---:|:------------------------------|-----------------------:|----------:|---------:|\n",
    "|  0 | resnet50                      |                    2   |  0.948073 | 0.838784 |\n",
    "|  1 | mocov3_resnet50               |                   10   |  0.966078 | 0.64681  |\n",
    "|  2 | vicreg_resnet50               |                   10   |  0.971546 | 0.616755 |\n",
    "|  3 | dino_resnet50                 |                    0.5 |  0.971055 | 0.590095 |\n",
    "|  4 | clip_RN50                     |                    0.5 |  0.979172 | 0.577304 |\n",
    "|  5 | vitb16                        |                    2   |  0.949133 | 0.889504 |\n",
    "|  6 | mocov3_vit_base               |                    0.5 |  0.874281 | 0.631526 |\n",
    "|  7 | timm_vit_base_patch16_224.mae |                    5   |  0.89496  | 0.29343  |\n",
    "|  8 | clip_vitb16                   |                    1   |  0.939767 | 0.661225 |\n",
    "\n",
    "Excluding cosine distance\n",
    "\n",
    "|    | model                         |   aggclust_dist_thresh |   rel-AMI |      AMI |\n",
    "|---:|:------------------------------|-----------------------:|----------:|---------:|\n",
    "|  0 | resnet50                      |                    2   |  0.948073 | 0.838784 |\n",
    "|  1 | mocov3_resnet50               |                   10   |  0.966078 | 0.64681  |\n",
    "|  2 | vicreg_resnet50               |                    0.5 |  0.951247 | 0.586766 |\n",
    "|  3 | dino_resnet50                 |                    0.5 |  0.971055 | 0.590095 |\n",
    "|  4 | clip_RN50                     |                    0.5 |  0.979172 | 0.577304 |\n",
    "|  5 | vitb16                        |                    2   |  0.946852 | 0.889504 |\n",
    "|  6 | mocov3_vit_base               |                    1   |  0.896512 | 0.655649 |\n",
    "|  7 | timm_vit_base_patch16_224.mae |                    5   |  0.89496  | 0.29343  |\n",
    "|  8 | dino_vitb16                   |                    0.2 |  0.905301 | 0.722863 |\n",
    "|  9 | clip_vitb16                   |                    1   |  0.939767 | 0.661225 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yrmctdctSRgu"
   },
   "outputs": [],
   "source": [
    "# Run AgglomerativeClustering experiments with number of clusters unknown\n",
    "# \tresnet50        \t20.0\n",
    "# \tmocov3_resnet50 \t20.0\n",
    "# \tvicreg_resnet50 \t20.0\n",
    "# \tvitb16 \t            20.0\n",
    "# \tdino_resnet50     \t 1.0\n",
    "# \tclip_RN50 \t         1.0\n",
    "# \tdino_vitb16 \t     2.0\n",
    "# \tmocov3_vit_base \t 1.0\n",
    "# \tclip_vitb16 \t     0.5\n",
    "# \ttimm_vit_base_patch16_224.mae \t200.0\n",
    "\n",
    "for model in [\"resnet50\", \"mocov3_resnet50\", \"vicreg_resnet50\", \"vitb16\"]:\n",
    "    BEST_PARAMS_v1[\"AgglomerativeClustering\"][model].update(\n",
    "        {\"aggclust_dist_thresh\": 20.0}\n",
    "    )\n",
    "for model in [\"dino_resnet50\", \"clip_RN50\", \"mocov3_vit_base\"]:\n",
    "    BEST_PARAMS_v1[\"AgglomerativeClustering\"][model].update(\n",
    "        {\"aggclust_dist_thresh\": 1.0}\n",
    "    )\n",
    "BEST_PARAMS_v1[\"AgglomerativeClustering\"][\"dino_vitb16\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v1[\"AgglomerativeClustering\"][\"clip_vitb16\"][\"aggclust_dist_thresh\"] = 0.5\n",
    "BEST_PARAMS_v1[\"AgglomerativeClustering\"][\"timm_vit_base_patch16_224.mae\"][\n",
    "    \"aggclust_dist_thresh\"\n",
    "] = 200.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nEW3rz_5aQ70"
   },
   "outputs": [],
   "source": [
    "BEST_PARAMS_v2[\"AgglomerativeClustering\"][\"resnet50\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v2[\"AgglomerativeClustering\"][\"mocov3_resnet50\"][\n",
    "    \"aggclust_dist_thresh\"\n",
    "] = 10.0\n",
    "BEST_PARAMS_v2[\"AgglomerativeClustering\"][\"vicreg_resnet50\"][\n",
    "    \"aggclust_dist_thresh\"\n",
    "] = 0.5\n",
    "BEST_PARAMS_v2[\"AgglomerativeClustering\"][\"dino_resnet50\"][\"aggclust_dist_thresh\"] = 0.5\n",
    "BEST_PARAMS_v2[\"AgglomerativeClustering\"][\"clip_RN50\"][\"aggclust_dist_thresh\"] = 0.5\n",
    "BEST_PARAMS_v2[\"AgglomerativeClustering\"][\"vitb16\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v2[\"AgglomerativeClustering\"][\"mocov3_vit_base\"][\n",
    "    \"aggclust_dist_thresh\"\n",
    "] = 1.0\n",
    "BEST_PARAMS_v2[\"AgglomerativeClustering\"][\"timm_vit_base_patch16_224.mae\"][\n",
    "    \"aggclust_dist_thresh\"\n",
    "] = 5.0\n",
    "BEST_PARAMS_v2[\"AgglomerativeClustering\"][\"dino_vitb16\"][\"aggclust_dist_thresh\"] = 0.2\n",
    "BEST_PARAMS_v2[\"AgglomerativeClustering\"][\"clip_vitb16\"][\"aggclust_dist_thresh\"] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t1WAi6d_i3tA"
   },
   "outputs": [],
   "source": [
    "for model in RESNET50_MODELS + VITB16_MODELS:\n",
    "    BEST_PARAMS_v2[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"aggclust_dist_thresh\": aggthresh_choices_df[\n",
    "                aggthresh_choices_df[\"model\"] == model\n",
    "            ][\"aggclust_dist_thresh\"].item(),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jXT7HoSsnk_"
   },
   "source": [
    "## Affinity Prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDoxPzukt9rv"
   },
   "source": [
    "### Convergence threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BAOWJ4zusqpb"
   },
   "outputs": [],
   "source": [
    "models = RESNET50_MODELS + VITB16_MODELS\n",
    "cmap = categorical_cmap(len(models), len(VALIDATION_DATASETS))\n",
    "clusterer = \"AffinityPropagation\"\n",
    "convergence_thresholds = [15, 20, 30, 45, 60, 90]\n",
    "\n",
    "override_fields = {}\n",
    "\n",
    "data = np.NaN * np.ones(\n",
    "    (len(models), len(VALIDATION_DATASETS), len(convergence_thresholds))\n",
    ")\n",
    "cmds = []\n",
    "for i_model, model in enumerate(models):\n",
    "    for i_dataset, dataset in enumerate(VALIDATION_DATASETS):\n",
    "        for i_thr, thr in enumerate(convergence_thresholds):\n",
    "            filter = {\n",
    "                \"model\": model,\n",
    "                \"dataset\": dataset,\n",
    "                \"clusterer\": clusterer,\n",
    "                \"affinity_conv_iter\": thr,\n",
    "            }\n",
    "            filter.update(override_fields)\n",
    "            sdf = select_rows(runs_df, filter, allow_missing=False)\n",
    "            filter2 = dict(DEFAULT_PARAMS[\"all\"], **BEST_PARAMS[clusterer][model])\n",
    "            filter2 = {k: v for k, v in filter2.items() if k not in filter}\n",
    "            sdf = select_rows(sdf, filter2, allow_missing=False)\n",
    "            if len(sdf) < 1:\n",
    "                if dataset == \"imagenet\":\n",
    "                    continue\n",
    "                print(f\"No data for {filter} {filter2}\")\n",
    "                cmds.append(filter2command(filter, filter2))\n",
    "                continue\n",
    "            if len(sdf) > 1:\n",
    "                if sum(sdf[\"AMI\"] != sdf.iloc[0][\"AMI\"]) > 0:\n",
    "                    print()\n",
    "                    print(\"More than one result with AMIs:\", list(sdf[\"AMI\"]))\n",
    "                    print(f\"for search {filter}\\nand {filter2}\")\n",
    "                    dif_cols = find_differing_columns(sdf, config_keys)\n",
    "                    print(f\"columns which differ: {dif_cols}\")\n",
    "                    if dif_cols:\n",
    "                        for col in dif_cols:\n",
    "                            print(f\"  {col}: {list(sdf[col])}\")\n",
    "            data[i_model, i_dataset, i_thr] = np.median(sdf[\"AMI\"])\n",
    "\n",
    "if len(cmds) > 0:\n",
    "    print()\n",
    "for cmd in cmds:\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pW0QMkNIuYB9"
   },
   "outputs": [],
   "source": [
    "affthresh_choices_rows = []\n",
    "\n",
    "for i_model, model in enumerate(models):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    ax = plt.axes()\n",
    "    # indiv\n",
    "    for i_dataset, dataset in enumerate(VALIDATION_DATASETS):\n",
    "        ls = DATASET2LS.get(dataset)\n",
    "        ls = ls + (\"s\" if dataset == \"imagenet\" else \"o\")\n",
    "        plt.plot(\n",
    "            convergence_thresholds,\n",
    "            data[i_model, i_dataset],\n",
    "            ls,\n",
    "            markersize=5,\n",
    "            color=\"grey\",\n",
    "        )\n",
    "    # mean\n",
    "    mu_data = np.nanmean(data[i_model], axis=0)\n",
    "    if np.all(np.isnan(mu_data)):\n",
    "        print(f\"No data for {model}\")\n",
    "        continue\n",
    "    i_thr = np.nanargmax(mu_data)\n",
    "    best_thr = convergence_thresholds[i_thr]\n",
    "    plt.plot(best_thr, mu_data[i_thr], \"x\")\n",
    "    plt.plot(convergence_thresholds, mu_data, color=\"black\")\n",
    "    plt.title(f\"{model} (thr={best_thr}, AMI={mu_data[i_thr]})\")\n",
    "    # plt.xscale(\"log\")\n",
    "    plt.xlabel(\"Convergence threshold\")\n",
    "    plt.ylabel(\"AMI\")\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    # plt.xscale(\"log\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    row = {\"model\": model, \"affinity_conv_iter\": best_thr}\n",
    "    affthresh_choices_rows.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qx8fy-3VuBX_"
   },
   "source": [
    "### Damping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rdmUxVxAuGnG"
   },
   "outputs": [],
   "source": [
    "models = RESNET50_MODELS + VITB16_MODELS\n",
    "cmap = categorical_cmap(len(models), len(VALIDATION_DATASETS))\n",
    "clusterer = \"AffinityPropagation\"\n",
    "damping_values = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "affinity_conv_iter = 90\n",
    "\n",
    "override_fields = {}\n",
    "\n",
    "data = np.NaN * np.ones((len(models), len(VALIDATION_DATASETS), len(damping_values)))\n",
    "cmds = []\n",
    "for i_model, model in enumerate(models):\n",
    "    for i_dataset, dataset in enumerate(VALIDATION_DATASETS):\n",
    "        for i_damping, damping in enumerate(damping_values):\n",
    "            filter = {\n",
    "                \"model\": model,\n",
    "                \"dataset\": dataset,\n",
    "                \"clusterer\": clusterer,\n",
    "                \"affinity_conv_iter\": affinity_conv_iter,\n",
    "                \"affinity_damping\": damping,\n",
    "            }\n",
    "            filter.update(override_fields)\n",
    "            sdf = select_rows(runs_df, filter, allow_missing=False)\n",
    "            filter2 = dict(DEFAULT_PARAMS[\"all\"], **BEST_PARAMS[clusterer][model])\n",
    "            filter2 = {k: v for k, v in filter2.items() if k not in filter}\n",
    "            sdf = select_rows(sdf, filter2, allow_missing=False)\n",
    "            if len(sdf) < 1:\n",
    "                if dataset == \"imagenet\":\n",
    "                    continue\n",
    "                print(f\"No data for {filter} {filter2}\")\n",
    "                cmds.append(filter2command(filter, filter2))\n",
    "                continue\n",
    "            if len(sdf) > 1:\n",
    "                if sum(sdf[\"AMI\"] != sdf.iloc[0][\"AMI\"]) > 0:\n",
    "                    print()\n",
    "                    print(\"More than one result with AMIs:\", list(sdf[\"AMI\"]))\n",
    "                    print(f\"for search {filter}\\nand {filter2}\")\n",
    "                    dif_cols = find_differing_columns(sdf, config_keys)\n",
    "                    print(f\"columns which differ: {dif_cols}\")\n",
    "                    if dif_cols:\n",
    "                        for col in dif_cols:\n",
    "                            print(f\"  {col}: {list(sdf[col])}\")\n",
    "            data[i_model, i_dataset, i_damping] = np.median(sdf[\"AMI\"])\n",
    "\n",
    "if len(cmds) > 0:\n",
    "    print()\n",
    "for cmd in cmds:\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BLlIapDZuJJr"
   },
   "outputs": [],
   "source": [
    "affdamping_choices_rows = []\n",
    "\n",
    "for i_model, model in enumerate(models):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    ax = plt.axes()\n",
    "    # indiv\n",
    "    for i_dataset, dataset in enumerate(VALIDATION_DATASETS):\n",
    "        ls = DATASET2LS.get(dataset)\n",
    "        ls = ls + (\"s\" if dataset == \"imagenet\" else \"o\")\n",
    "        plt.plot(\n",
    "            damping_values, data[i_model, i_dataset], ls, markersize=5, color=\"grey\"\n",
    "        )\n",
    "    # mean\n",
    "    mu_data = np.nanmean(data[i_model], axis=0)\n",
    "    if np.all(np.isnan(mu_data)):\n",
    "        print(f\"No data for {model}\")\n",
    "        continue\n",
    "    i_thr = np.nanargmax(mu_data)\n",
    "    best_thr = damping_values[i_thr]\n",
    "    plt.plot(best_thr, mu_data[i_thr], \"x\")\n",
    "    plt.plot(damping_values, mu_data, color=\"black\")\n",
    "    plt.title(f\"{model} (thr={best_thr}, AMI={mu_data[i_thr]})\")\n",
    "    plt.xlabel(\"Damping\")\n",
    "    plt.ylabel(\"AMI\")\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    # plt.xscale(\"log\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    row = {\"model\": model, \"affinity_conv_iter\": best_thr}\n",
    "    affdamping_choices_rows.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whDbPMuIh5lL"
   },
   "source": [
    "## HDBSCAN redux\n",
    "\n",
    "Redo HDBSCAN method and metric selection with dim reduction in place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6IqLcZ-h-Rw"
   },
   "source": [
    "### EOM vs leaf and distance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jg670qBnh-Rw"
   },
   "outputs": [],
   "source": [
    "models = RESNET50_MODELS + VITB16_MODELS\n",
    "cmap = categorical_cmap(len(models), len(VALIDATION_DATASETS))\n",
    "clusterer = \"HDBSCAN\"\n",
    "methods = [\"eom\", \"leaf\"]\n",
    "metrics = [\"euclidean\", \"l1\", \"chebyshev\", \"arccos\"]  # , \"braycurtis\", \"canberra\"]\n",
    "\n",
    "data = np.NaN * np.ones(\n",
    "    (len(models), len(VALIDATION_DATASETS), len(methods), len(metrics))\n",
    ")\n",
    "cmds = []\n",
    "for i_model, model in enumerate(models):\n",
    "    for i_dataset, dataset in enumerate(VALIDATION_DATASETS):\n",
    "        for i_method, method in enumerate(methods):\n",
    "            for i_metric, metric in enumerate(metrics):\n",
    "                if metric == \"cosine\":\n",
    "                    continue\n",
    "                filter = {\n",
    "                    \"model\": model,\n",
    "                    \"dataset\": dataset,\n",
    "                    \"clusterer\": clusterer,\n",
    "                    \"distance_metric\": metric,\n",
    "                    \"hdbscan_method\": method,\n",
    "                }\n",
    "                sdf = select_rows(runs_df, filter, allow_missing=False)\n",
    "                filter2 = dict(DEFAULT_PARAMS[\"all\"], **BEST_PARAMS[clusterer][model])\n",
    "                filter2 = {k: v for k, v in filter2.items() if k not in filter}\n",
    "                sdf = select_rows(sdf, filter2, allow_missing=False)\n",
    "                if len(sdf) < 1:\n",
    "                    print(\"No data for\", filter)\n",
    "                    cmds.append(filter2command(filter, filter2))\n",
    "                    continue\n",
    "                if len(sdf) > 1:\n",
    "                    if sum(sdf[\"AMI\"] != sdf.iloc[0][\"AMI\"]) > 0:\n",
    "                        print()\n",
    "                        print(\"More than one result with AMIs:\", list(sdf[\"AMI\"]))\n",
    "                        print(f\"for search {filter} and {filter2}\")\n",
    "                        dif_cols = find_differing_columns(sdf, config_keys)\n",
    "                        print(f\"columns which differ: {dif_cols}\")\n",
    "                        if dif_cols:\n",
    "                            for col in dif_cols:\n",
    "                                print(f\"  {col}: {list(sdf[col])}\")\n",
    "                data[i_model, i_dataset, i_method, i_metric] = np.median(sdf[\"AMI\"])\n",
    "\n",
    "for cmd in cmds:\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8J8ckbLgh-Rx"
   },
   "outputs": [],
   "source": [
    "np.mean(np.mean(data, axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhFEr6LWh-Rx"
   },
   "outputs": [],
   "source": [
    "np.nanmean(np.nanmean(data, axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iGhUCt3th-Ry"
   },
   "outputs": [],
   "source": [
    "data[0, 0, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WytYpK32h-Ry"
   },
   "outputs": [],
   "source": [
    "max_data = np.nanmax(data)\n",
    "YLIM = [-0.05 * max_data, 1.05 * max_data]\n",
    "for i_method, method in enumerate(methods):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    ax = plt.axes()\n",
    "    i = 0\n",
    "    for i_model, model in enumerate(RESNET50_MODELS):\n",
    "        for i_dataset, dataset in enumerate(VALIDATION_DATASETS):\n",
    "            plt.plot(\n",
    "                data[i_model, i_dataset, i_method, :],\n",
    "                \"x\",\n",
    "                label=f\"{dataset}: {model}\",\n",
    "                c=cmap(i),\n",
    "            )\n",
    "            i += 1\n",
    "    # plt.legend()\n",
    "    ax.set_xticks(np.arange(len(metrics)), metrics)\n",
    "    plt.ylim(YLIM)\n",
    "    plt.title(method)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lTCFsj6ch-Ry"
   },
   "outputs": [],
   "source": [
    "width = 0.02\n",
    "YLIM = [-0.05, 1.05]\n",
    "for i_method, method in enumerate(methods):\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    ax = plt.axes()\n",
    "    i = 0\n",
    "    for i_model, model in enumerate(models):\n",
    "        for i_dataset, dataset in enumerate(VALIDATION_DATASETS):\n",
    "            plt.bar(\n",
    "                np.arange(len(metrics)) + i * width,\n",
    "                data[i_model, i_dataset, i_method, :],\n",
    "                width=width,\n",
    "                label=f\"{dataset}: {model}\",\n",
    "                color=cmap(i),\n",
    "            )\n",
    "            i += 1\n",
    "    # plt.legend()\n",
    "    ax.set_xticks(np.arange(len(metrics)) + width * (i + 1) / 2, metrics)\n",
    "    plt.ylim(YLIM)\n",
    "    plt.title(method)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IgRM5GIWmoDt"
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MdVhzfablyJ4"
   },
   "outputs": [],
   "source": [
    "width = 0.1\n",
    "YLIM = [-0.05, 1.05]\n",
    "cmap = categorical_cmap(len(methods), len(metrics))\n",
    "for i_model, model in enumerate(models):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    ax = plt.axes()\n",
    "    i = 0\n",
    "    for i_dataset, dataset in enumerate(VALIDATION_DATASETS):\n",
    "        for i_method, method in enumerate(methods):\n",
    "            for i_metric, metric in enumerate(metrics):\n",
    "                plt.bar(\n",
    "                    i_dataset + (i_metric + (i_method * len(metrics))) * width,\n",
    "                    data[i_model, i_dataset, i_method, i_metric],\n",
    "                    width=width,\n",
    "                    label=f\"{dataset}: {method}, {metric}\",\n",
    "                    color=cmap(i_metric + (i_method * len(metrics))),\n",
    "                )\n",
    "                i += 1\n",
    "    # plt.legend()\n",
    "    ax.set_xticks(\n",
    "        np.arange(len(VALIDATION_DATASETS)) + width * (len(metrics) - 0.5),\n",
    "        VALIDATION_DATASETS,\n",
    "    )\n",
    "    plt.ylim(YLIM)\n",
    "    plt.ylabel(\"AMI\")\n",
    "    plt.title(model)\n",
    "    plt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LfHyPCoynR0h"
   },
   "outputs": [],
   "source": [
    "# weights_val = np.ones((len(VALIDATION_DATASETS), ), dtype=int)\n",
    "weights_val = np.array([2 if d == \"imagenet\" else 1 for d in VALIDATION_DATASETS])\n",
    "\n",
    "hdbscan_choices_rows = []\n",
    "width = 0.1\n",
    "\n",
    "avg_ami = np.average(data, axis=1, weights=weights_val)\n",
    "for i_model, model in enumerate(models):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    min_ami = 1\n",
    "    max_ami = 0\n",
    "    ax = plt.axes()\n",
    "    i = 0\n",
    "    best_str = \"NO DATA\"\n",
    "    row = {}\n",
    "    for i_method, method in enumerate(methods):\n",
    "        my_val = avg_ami[i_model, i_method, :]\n",
    "        plt.bar(\n",
    "            np.arange(len(metrics)) + i * width,\n",
    "            my_val,\n",
    "            width=width * len(VALIDATION_DATASETS),\n",
    "            label=f\"{model} {method}\",\n",
    "            color=cmap(i),\n",
    "        )\n",
    "        my_val_best = np.nanmax(my_val)\n",
    "        min_ami = np.nanmin([min_ami, np.nanmin(my_val)])\n",
    "        max_ami = np.nanmax([max_ami, my_val_best])\n",
    "        i += len(methods)\n",
    "\n",
    "    # Exclude arccos distance because it doesn't make sense with UMAP generally\n",
    "    # (the origin is ill-defined)\n",
    "    best_method_idx, best_metric_idx = np.unravel_index(\n",
    "        np.nanargmax(avg_ami[i_model, :, :3]),\n",
    "        avg_ami[i_model, :, :3].shape,\n",
    "    )\n",
    "    best_ami = avg_ami[i_model, best_method_idx, best_metric_idx]\n",
    "    ax.set_xticks(\n",
    "        np.arange(len(metrics)) + width * (i - len(VALIDATION_DATASETS)) / 2, metrics\n",
    "    )\n",
    "    YLIM = np.array([min_ami, max_ami])\n",
    "    YLIM += np.array([-1, 1]) * 0.05 * (YLIM[1] - YLIM[0])\n",
    "    plt.ylim(YLIM)\n",
    "    plt.ylabel(\"AMI\")\n",
    "    top_k = np.sort(avg_ami[i_model][~np.isnan(avg_ami[i_model])], axis=None)[-2:-4:-1]\n",
    "    plt.title(\n",
    "        f\"{model} : {metrics[best_metric_idx]} {methods[best_method_idx]}\"\n",
    "        f\"  (AMI={best_ami:.3f})  ... {top_k}\"\n",
    "    )\n",
    "    plt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()\n",
    "    row = {\n",
    "        \"model\": model,\n",
    "        \"distance_metric\": metrics[best_metric_idx],\n",
    "        \"hdbscan_method\": methods[best_method_idx],\n",
    "        \"AMI\": best_ami,\n",
    "    }\n",
    "    hdbscan_choices_rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l8B5CbvJqpnV"
   },
   "outputs": [],
   "source": [
    "# weights_val = np.ones((len(VALIDATION_DATASETS), ), dtype=int)\n",
    "weights_val = np.array([2 if d == \"imagenet\" else 1 for d in VALIDATION_DATASETS])\n",
    "\n",
    "width = 0.13\n",
    "\n",
    "avg_ami = np.average(data, axis=1, weights=weights_val)\n",
    "for i_model, model in enumerate(models):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    min_ami = 1\n",
    "    max_ami = 0\n",
    "    ax = plt.axes()\n",
    "    i = 0\n",
    "    best_str = \"NO DATA\"\n",
    "    for i_method, method in enumerate(methods[:1]):\n",
    "        my_val = avg_ami[i_model, i_method, :]\n",
    "        plt.bar(\n",
    "            np.arange(len(metrics)) + i * width,\n",
    "            my_val,\n",
    "            width=width * len(VALIDATION_DATASETS),\n",
    "            label=f\"{dataset}: {model}\",\n",
    "            color=cmap(i),\n",
    "        )\n",
    "        my_val_best = np.nanmax(my_val)\n",
    "        if my_val_best > max_ami:\n",
    "            best_metric_idx = np.nanargmax(my_val)\n",
    "            best_str = (\n",
    "                f\"{method} with {metrics[best_metric_idx]} (AMI={my_val_best:.5f})\"\n",
    "            )\n",
    "        min_ami = np.nanmin([min_ami, np.nanmin(my_val)])\n",
    "        max_ami = np.nanmax([max_ami, my_val_best])\n",
    "        i += len(VALIDATION_DATASETS)\n",
    "    # plt.legend()\n",
    "    ax.set_xticks(\n",
    "        np.arange(len(metrics)) + width * (i - len(VALIDATION_DATASETS)) / 2, metrics\n",
    "    )\n",
    "    YLIM = np.array([min_ami, max_ami])\n",
    "    YLIM += np.array([-1, 1]) * 0.05 * (YLIM[1] - YLIM[0])\n",
    "    plt.ylim(YLIM)\n",
    "    plt.ylabel(\"AMI\")\n",
    "    top_k = np.sort(avg_ami[i_model][~np.isnan(avg_ami[i_model])], axis=None)[-2:-4:-1]\n",
    "    plt.title(f\"{model} : {best_str}  ... {top_k}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uyeiLDzLz6c9"
   },
   "outputs": [],
   "source": [
    "hdbscan_choices_df = pd.DataFrame.from_dict(hdbscan_choices_rows)\n",
    "hdbscan_choices_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QaEaUzT8z_Ap"
   },
   "outputs": [],
   "source": [
    "print(hdbscan_choices_df.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YHODpPwVL0FT"
   },
   "outputs": [],
   "source": [
    "for model in RESNET50_MODELS + VITB16_MODELS:\n",
    "    BEST_PARAMS_v1[\"HDBSCAN\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"euclidean\",\n",
    "            \"hdbscan_method\": \"eom\",\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77s1W-oVdPY-"
   },
   "source": [
    "v2 selection\n",
    "\n",
    "|    | model                         | distance_metric   | hdbscan_method   |      AMI |\n",
    "|---:|:------------------------------|:------------------|:-----------------|---------:|\n",
    "|  0 | resnet50                      | euclidean         | eom              | 0.828368 |\n",
    "|  1 | mocov3_resnet50               | euclidean         | eom              | 0.531644 |\n",
    "|  2 | vicreg_resnet50               | l1                | eom              | 0.472324 |\n",
    "|  3 | dino_resnet50                 | l1                | eom              | 0.503147 |\n",
    "|  4 | clip_RN50                     | l1                | eom              | 0.461363 |\n",
    "|  5 | vitb16                        | chebyshev         | eom              | 0.906110 |\n",
    "|  6 | mocov3_vit_base               | euclidean         | eom              | 0.629966 |\n",
    "|  7 | timm_vit_base_patch16_224.mae | euclidean         | eom              | 0.070495 |\n",
    "|  8 | dino_vitb16                   | l1                | eom              | 0.691547 |\n",
    "|  9 | clip_vitb16                   | l1                | eom              | 0.592489 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDMQjXpoKb6_"
   },
   "outputs": [],
   "source": [
    "for model in RESNET50_MODELS + VITB16_MODELS:\n",
    "    BEST_PARAMS_v2[\"HDBSCAN\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"euclidean\",\n",
    "            \"hdbscan_method\": \"eom\",\n",
    "        }\n",
    "    )\n",
    "for model in [\n",
    "    \"vicreg_resnet50\",\n",
    "    \"dino_resnet50\",\n",
    "    \"clip_RN50\",\n",
    "    \"dino_vitb16\",\n",
    "    \"clip_vitb16\",\n",
    "]:\n",
    "    BEST_PARAMS_v2[\"HDBSCAN\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"l1\",\n",
    "        }\n",
    "    )\n",
    "BEST_PARAMS_v2[\"HDBSCAN\"][model][\"distance_metric\"] = \"vitb16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nKYseOkOz8RU"
   },
   "outputs": [],
   "source": [
    "for model in RESNET50_MODELS + VITB16_MODELS:\n",
    "    BEST_PARAMS_v2[\"HDBSCAN\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": hdbscan_choices_df[hdbscan_choices_df[\"model\"] == model][\n",
    "                \"distance_metric\"\n",
    "            ].item(),\n",
    "            \"hdbscan_method\": hdbscan_choices_df[hdbscan_choices_df[\"model\"] == model][\n",
    "                \"hdbscan_method\"\n",
    "            ].item(),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TW8K28HKR8wZ"
   },
   "source": [
    "## OPTICS (WIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgstdY1FR8wb"
   },
   "source": [
    "### dbscan vs xi (with xi threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YpqaAX-JS_EC"
   },
   "outputs": [],
   "source": [
    "models = RESNET50_MODELS + VITB16_MODELS\n",
    "cmap = categorical_cmap(len(models), len(VALIDATION_DATASETS))\n",
    "clusterer = \"OPTICS\"\n",
    "metrics = [\"euclidean\", \"l1\", \"chebyshev\", \"cosine\"]\n",
    "distance_thresholds = [\n",
    "    0.01,\n",
    "    0.02,\n",
    "    0.03,\n",
    "    0.04,\n",
    "    0.05,\n",
    "    0.07,\n",
    "    0.1,\n",
    "    0.15,\n",
    "    0.2,\n",
    "    0.3,\n",
    "    0.4,\n",
    "    0.5,\n",
    "]\n",
    "\n",
    "# Run with standardization to make things more comparable across datasets\n",
    "override_fields = {\n",
    "    \"zscore2\": \"average\",\n",
    "    \"ndim_correction\": True,\n",
    "}\n",
    "# No standardization (original configuration)\n",
    "# override_fields = {}\n",
    "\n",
    "data = np.NaN * np.ones(\n",
    "    (len(models), len(VALIDATION_DATASETS), len(metrics), len(distance_thresholds))\n",
    ")\n",
    "cmds = []\n",
    "for i_model, model in enumerate(models):\n",
    "    for i_dataset, dataset in enumerate(VALIDATION_DATASETS):\n",
    "        for i_metric, metric in enumerate(metrics):\n",
    "            for i_thr, thr in enumerate(distance_thresholds):\n",
    "                filter = {\n",
    "                    \"model\": model,\n",
    "                    \"dataset\": dataset,\n",
    "                    \"clusterer\": clusterer,\n",
    "                    \"distance_metric\": metric,\n",
    "                    \"optics_method\": \"xi\",\n",
    "                    \"aggclust_dist_thresh\": thr,\n",
    "                }\n",
    "                filter.update(override_fields)\n",
    "                sdf = select_rows(runs_df, filter, allow_missing=False)\n",
    "                filter2 = dict(DEFAULT_PARAMS[\"all\"], **BEST_PARAMS[clusterer][model])\n",
    "                filter2 = {k: v for k, v in filter2.items() if k not in filter}\n",
    "                sdf = select_rows(sdf, filter2, allow_missing=False)\n",
    "                if len(sdf) < 1:\n",
    "                    continue\n",
    "                    print(f\"No data for {filter} {filter2}\")\n",
    "                    cmds.append(filter2command(filter, filter2))\n",
    "                    continue\n",
    "                if len(sdf) > 1:\n",
    "                    if sum(sdf[\"AMI\"] != sdf.iloc[0][\"AMI\"]) > 0:\n",
    "                        print()\n",
    "                        print(\"More than one result with AMIs:\", list(sdf[\"AMI\"]))\n",
    "                        print(f\"for search {filter}\\nand {filter2}\")\n",
    "                        dif_cols = find_differing_columns(sdf, config_keys)\n",
    "                        print(f\"columns which differ: {dif_cols}\")\n",
    "                        if dif_cols:\n",
    "                            for col in dif_cols:\n",
    "                                print(f\"  {col}: {list(sdf[col])}\")\n",
    "                data[i_model, i_dataset, i_metric, i_thr] = np.median(sdf[\"AMI\"])\n",
    "\n",
    "if len(cmds) > 0:\n",
    "    print()\n",
    "for cmd in cmds:\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J1GKvWxZR8wb"
   },
   "outputs": [],
   "source": [
    "models = RESNET50_MODELS + VITB16_MODELS\n",
    "cmap = categorical_cmap(len(models), len(VALIDATION_DATASETS))\n",
    "clusterer = \"OPTICS\"\n",
    "methods = [\"dbscan\", \"xi\"]\n",
    "metrics = [\"euclidean\", \"l1\", \"chebyshev\", \"cosine\"]\n",
    "\n",
    "data = np.NaN * np.ones(\n",
    "    (len(models), len(VALIDATION_DATASETS), len(methods), len(metrics))\n",
    ")\n",
    "cmds = []\n",
    "for i_model, model in enumerate(models):\n",
    "    for i_dataset, dataset in enumerate(VALIDATION_DATASETS):\n",
    "        for i_method, method in enumerate(methods):\n",
    "            for i_metric, metric in enumerate(metrics):\n",
    "                filter = {\n",
    "                    \"model\": model,\n",
    "                    \"dataset\": dataset,\n",
    "                    \"clusterer\": clusterer,\n",
    "                    \"distance_metric\": metric,\n",
    "                    \"optics_method\": method,\n",
    "                }\n",
    "                if metric == \"xi\":\n",
    "                    filter[\"optics_xi\"] = 0.05\n",
    "                else:\n",
    "                    filter[\"optics_xi\"] = None\n",
    "                sdf = select_rows(runs_df, filter, allow_missing=False)\n",
    "                filter2 = dict(DEFAULT_PARAMS[\"all\"], **BEST_PARAMS[clusterer][model])\n",
    "                filter2 = {k: v for k, v in filter2.items() if k not in filter}\n",
    "                sdf = select_rows(sdf, filter2, allow_missing=False)\n",
    "                if len(sdf) < 1:\n",
    "                    print(\"No data for\", filter)\n",
    "                    cmds.append(filter2command(filter, filter2))\n",
    "                    continue\n",
    "                if len(sdf) > 1:\n",
    "                    if sum(sdf[\"AMI\"] != sdf.iloc[0][\"AMI\"]) > 0:\n",
    "                        print()\n",
    "                        print(\"More than one result with AMIs:\", list(sdf[\"AMI\"]))\n",
    "                        print(f\"for search {filter} and {filter2}\")\n",
    "                        dif_cols = find_differing_columns(sdf, config_keys)\n",
    "                        print(f\"columns which differ: {dif_cols}\")\n",
    "                        if dif_cols:\n",
    "                            for col in dif_cols:\n",
    "                                print(f\"  {col}: {list(sdf[col])}\")\n",
    "                data[i_model, i_dataset, i_method, i_metric] = np.median(sdf[\"AMI\"])\n",
    "\n",
    "for cmd in cmds:\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLKUMScVR8wc"
   },
   "outputs": [],
   "source": [
    "np.mean(np.mean(data, axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F63xxWmPR8wd"
   },
   "outputs": [],
   "source": [
    "np.nanmean(np.nanmean(data, axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vCRyeipTR8wd"
   },
   "outputs": [],
   "source": [
    "data[0, 0, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gK29168vR8we"
   },
   "outputs": [],
   "source": [
    "max_data = np.nanmax(data)\n",
    "YLIM = [-0.05 * max_data, 1.05 * max_data]\n",
    "for i_method, method in enumerate(methods):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    ax = plt.axes()\n",
    "    i = 0\n",
    "    for i_model, model in enumerate(RESNET50_MODELS):\n",
    "        for i_dataset, dataset in enumerate(VALIDATION_DATASETS):\n",
    "            plt.plot(\n",
    "                data[i_model, i_dataset, i_method, :],\n",
    "                \"x\",\n",
    "                label=f\"{dataset}: {model}\",\n",
    "                c=cmap(i),\n",
    "            )\n",
    "            i += 1\n",
    "    # plt.legend()\n",
    "    ax.set_xticks(np.arange(len(metrics)), metrics)\n",
    "    plt.ylim(YLIM)\n",
    "    plt.title(method)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8urRUQ61R8we"
   },
   "outputs": [],
   "source": [
    "width = 0.02\n",
    "YLIM = [-0.05, 1.05]\n",
    "for i_method, method in enumerate(methods):\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    ax = plt.axes()\n",
    "    i = 0\n",
    "    for i_model, model in enumerate(models):\n",
    "        for i_dataset, dataset in enumerate(VALIDATION_DATASETS):\n",
    "            plt.bar(\n",
    "                np.arange(len(metrics)) + i * width,\n",
    "                data[i_model, i_dataset, i_method, :],\n",
    "                width=width,\n",
    "                label=f\"{dataset}: {model}\",\n",
    "                color=cmap(i),\n",
    "            )\n",
    "            i += 1\n",
    "    # plt.legend()\n",
    "    ax.set_xticks(np.arange(len(metrics)) + width * (i + 1) / 2, metrics)\n",
    "    plt.ylim(YLIM)\n",
    "    plt.title(method)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XWQX1efBR8we"
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CoyhFDPGR8wf"
   },
   "outputs": [],
   "source": [
    "width = 0.12\n",
    "YLIM = [-0.05, 1.05]\n",
    "for i_model, model in enumerate(models):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    ax = plt.axes()\n",
    "    i = 0\n",
    "    for i_method, method in enumerate(methods):\n",
    "        for i_dataset, dataset in enumerate(VALIDATION_DATASETS):\n",
    "            plt.bar(\n",
    "                np.arange(len(metrics)) + i * width,\n",
    "                data[i_model, i_dataset, i_method, :],\n",
    "                width=width,\n",
    "                label=f\"{dataset}: {method}\",\n",
    "                color=cmap(i),\n",
    "            )\n",
    "            i += 1\n",
    "    # plt.legend()\n",
    "    ax.set_xticks(np.arange(len(metrics)) + width * (i - 1) / 2, metrics)\n",
    "    plt.ylim(YLIM)\n",
    "    plt.ylabel(\"AMI\")\n",
    "    plt.title(model)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UCiwE8fuR8wf"
   },
   "outputs": [],
   "source": [
    "# weights_val = np.ones((len(VALIDATION_DATASETS), ), dtype=int)\n",
    "weights_val = np.array([2 if d == \"imagenet\" else 1 for d in VALIDATION_DATASETS])\n",
    "\n",
    "hdbscan_choices_rows = []\n",
    "width = 0.13\n",
    "\n",
    "avg_ami = np.average(data, axis=1, weights=weights_val)\n",
    "for i_model, model in enumerate(models):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    min_ami = 1\n",
    "    max_ami = 0\n",
    "    ax = plt.axes()\n",
    "    i = 0\n",
    "    best_str = \"NO DATA\"\n",
    "    row = {}\n",
    "    for i_method, method in enumerate(methods):\n",
    "        my_val = avg_ami[i_model, i_method, :]\n",
    "        plt.bar(\n",
    "            np.arange(len(metrics)) + i * width,\n",
    "            my_val,\n",
    "            width=width * len(VALIDATION_DATASETS),\n",
    "            label=f\"{model} {method}\",\n",
    "            color=cmap(i),\n",
    "        )\n",
    "        my_val_best = np.nanmax(my_val)\n",
    "        min_ami = np.nanmin([min_ami, np.nanmin(my_val)])\n",
    "        max_ami = np.nanmax([max_ami, my_val_best])\n",
    "        i += len(VALIDATION_DATASETS)\n",
    "\n",
    "    best_method_idx, best_metric_idx = np.unravel_index(\n",
    "        np.nanargmax(avg_ami[i_model]),\n",
    "        avg_ami[i_model].shape,\n",
    "    )\n",
    "    ax.set_xticks(\n",
    "        np.arange(len(metrics)) + width * (i - len(VALIDATION_DATASETS)) / 2, metrics\n",
    "    )\n",
    "    YLIM = np.array([min_ami, max_ami])\n",
    "    YLIM += np.array([-1, 1]) * 0.05 * (YLIM[1] - YLIM[0])\n",
    "    plt.ylim(YLIM)\n",
    "    plt.ylabel(\"AMI\")\n",
    "    top_k = np.sort(avg_ami[i_model][~np.isnan(avg_ami[i_model])], axis=None)[-2:-4:-1]\n",
    "    plt.title(\n",
    "        f\"{model} : {metrics[best_metric_idx]} {methods[best_method_idx]}\"\n",
    "        f\"  (AMI={avg_ami[i_model, best_method_idx, best_metric_idx]:.3f})\"\n",
    "        f\"  ... {top_k}\"\n",
    "    )\n",
    "    plt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()\n",
    "    row = {\n",
    "        \"model\": model,\n",
    "        \"distance_metric\": metrics[best_metric_idx],\n",
    "        \"hdbscan_method\": methods[best_method_idx],\n",
    "    }\n",
    "    hdbscan_choices_rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FyGZo_HIR8wg"
   },
   "outputs": [],
   "source": [
    "# weights_val = np.ones((len(VALIDATION_DATASETS), ), dtype=int)\n",
    "weights_val = np.array([2 if d == \"imagenet\" else 1 for d in VALIDATION_DATASETS])\n",
    "\n",
    "width = 0.13\n",
    "\n",
    "avg_ami = np.average(data, axis=1, weights=weights_val)\n",
    "for i_model, model in enumerate(models):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    min_ami = 1\n",
    "    max_ami = 0\n",
    "    ax = plt.axes()\n",
    "    i = 0\n",
    "    best_str = \"NO DATA\"\n",
    "    for i_method, method in enumerate(methods[:1]):\n",
    "        my_val = avg_ami[i_model, i_method, :]\n",
    "        plt.bar(\n",
    "            np.arange(len(metrics)) + i * width,\n",
    "            my_val,\n",
    "            width=width * len(VALIDATION_DATASETS),\n",
    "            label=f\"{dataset}: {model}\",\n",
    "            color=cmap(i),\n",
    "        )\n",
    "        my_val_best = np.nanmax(my_val)\n",
    "        if my_val_best > max_ami:\n",
    "            best_metric_idx = np.nanargmax(my_val)\n",
    "            best_str = (\n",
    "                f\"{method} with {metrics[best_metric_idx]} (AMI={my_val_best:.5f})\"\n",
    "            )\n",
    "        min_ami = np.nanmin([min_ami, np.nanmin(my_val)])\n",
    "        max_ami = np.nanmax([max_ami, my_val_best])\n",
    "        i += len(VALIDATION_DATASETS)\n",
    "    # plt.legend()\n",
    "    ax.set_xticks(\n",
    "        np.arange(len(metrics)) + width * (i - len(VALIDATION_DATASETS)) / 2, metrics\n",
    "    )\n",
    "    YLIM = np.array([min_ami, max_ami])\n",
    "    YLIM += np.array([-1, 1]) * 0.05 * (YLIM[1] - YLIM[0])\n",
    "    plt.ylim(YLIM)\n",
    "    plt.ylabel(\"AMI\")\n",
    "    top_k = np.sort(avg_ami[i_model][~np.isnan(avg_ami[i_model])], axis=None)[-2:-4:-1]\n",
    "    plt.title(f\"{model} : {best_str}  ... {top_k}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lOKY2NsOR8wg"
   },
   "outputs": [],
   "source": [
    "hdbscan_choices_df = pd.DataFrame.from_dict(hdbscan_choices_rows)\n",
    "hdbscan_choices_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vnK7Q_1WR8wh"
   },
   "outputs": [],
   "source": [
    "print(hdbscan_choices_df.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3nU4Fi82R8wh"
   },
   "outputs": [],
   "source": [
    "for model in RESNET50_MODELS + VITB16_MODELS:\n",
    "    BEST_PARAMS[\"HDBSCAN\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": hdbscan_choices_df[hdbscan_choices_df[\"model\"] == model][\n",
    "                \"distance_metric\"\n",
    "            ].item(),\n",
    "            \"hdbscan_method\": hdbscan_choices_df[hdbscan_choices_df[\"model\"] == model][\n",
    "                \"hdbscan_method\"\n",
    "            ].item(),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOKljXoMNGG7"
   },
   "source": [
    "# Final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QZYGEjWvWWO2"
   },
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CuPbp58ONv4Z"
   },
   "outputs": [],
   "source": [
    "TEST_DATASETS = [\n",
    "    \"imagenet\",\n",
    "    \"cifar10\",\n",
    "    \"cifar100\",\n",
    "    \"mnist\",\n",
    "    \"fashionmnist\",\n",
    "    \"svhn\",\n",
    "    \"flowers102\",\n",
    "    \"aircraft\",\n",
    "    \"nabirds\",\n",
    "    \"inaturalist\",\n",
    "]\n",
    "DATASET2SH = {\n",
    "    \"aircraft\": \"Aircraft\",\n",
    "    \"cifar10\": \"C10\",\n",
    "    \"cifar100\": \"C100\",\n",
    "    \"flowers102\": \"Flowers\",\n",
    "    \"fashionmnist\": \"fMNIST\",\n",
    "    \"imagenet\": \"IN1k\",\n",
    "    \"imagenette\": \"IN10\",\n",
    "    \"imagewoof\": \"INwf\",\n",
    "    \"inaturalist\": \"iNat21\",\n",
    "    \"mnist\": \"MNIST\",\n",
    "    \"nabirds\": \"NABirds\",\n",
    "    \"svhn\": \"SVHN\",\n",
    "}\n",
    "MODEL_GROUPS = {\n",
    "    \"ResNet-50\": RESNET50_MODELS,\n",
    "    \"ViT-B\": VITB16_MODELS,\n",
    "}\n",
    "MODEL2SH = {\n",
    "    \"resnet50\": \"Supervised\",\n",
    "    \"mocov3_resnet50\": \"MoCo-v3\",\n",
    "    \"vicreg_resnet50\": \"VICReg\",\n",
    "    \"dino_resnet50\": \"DINO\",\n",
    "    \"clip_RN50\": \"CLIP\",\n",
    "    \"vitb16\": \"Supervised\",\n",
    "    \"mocov3_vit_base\": \"MoCo-v3\",\n",
    "    \"timm_vit_base_patch16_224.mae\": \"MAE\",\n",
    "    \"dino_vitb16\": \"DINO\",\n",
    "    \"clip_vitb16\": \"CLIP\",\n",
    "}\n",
    "CLUSTERER2SH = {\n",
    "    \"KMeans\": \"K-Means\",\n",
    "    \"AffinityPropagation\": \"Affinity Prop\",\n",
    "    \"AgglomerativeClustering\": \"AC\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GRNXaeD-tWUo"
   },
   "source": [
    "## Fetch results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "id": "dytOzTA3NMeF",
    "outputId": "7ee74b21-d42d-4762-8d7a-4de0aedd8b5a"
   },
   "outputs": [],
   "source": [
    "# Project is specified by <entity/project-name>\n",
    "api = wandb.Api()\n",
    "runs_test = api.runs(\n",
    "    \"uoguelph_mlrg/zs-ssl-clustering\",\n",
    "    filters={\"state\": \"Finished\", \"config.partition\": \"test\"},\n",
    ")\n",
    "len(runs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "522p4kwrJd06"
   },
   "outputs": [],
   "source": [
    "summary_list, config_list, name_list = [], [], []\n",
    "for run in runs_test:\n",
    "    # .summary contains the output keys/values for metrics like accuracy.\n",
    "    #  We call ._json_dict to omit large files\n",
    "    summary_list.append(run.summary._json_dict)\n",
    "    # .config contains the hyperparameters.\n",
    "    #  We remove special values that start with _.\n",
    "    config_list.append({k: v for k, v in run.config.items() if not k.startswith(\"_\")})\n",
    "    # .name is the human-readable name of the run.\n",
    "    name_list.append(run.name)\n",
    "\n",
    "rows = []\n",
    "config_keys = set()\n",
    "summary_keys = set()\n",
    "for summary, config, name in zip(summary_list, config_list, name_list):\n",
    "    row = {\"name\": name}\n",
    "    row.update({k: v for k, v in config.items() if not k.startswith(\"_\")})\n",
    "    row.update({k: v for k, v in summary.items() if not k.startswith(\"_\")})\n",
    "    row[\"_timestamp\"] = summary[\"_timestamp\"]\n",
    "    rows.append(row)\n",
    "    config_keys = config_keys.union(config.keys())\n",
    "    summary_keys = summary_keys.union(summary.keys())\n",
    "\n",
    "test_runs_df = pd.DataFrame.from_records(rows)\n",
    "\n",
    "# Handle changed default value for spectral_assigner after config arg was introduced\n",
    "if \"spectral_assigner\" not in test_runs_df.columns:\n",
    "    test_runs_df[\"spectral_assigner\"] = None\n",
    "select = test_runs_df[\"clusterer_name\"] != \"SpectralClustering\"\n",
    "test_runs_df.loc[select, \"spectral_assigner\"] = None\n",
    "select = (test_runs_df[\"clusterer_name\"] == \"SpectralClustering\") & pd.isna(\n",
    "    test_runs_df[\"spectral_assigner\"]\n",
    ")\n",
    "test_runs_df.loc[select, \"spectral_assigner\"] = \"kmeans\"\n",
    "\n",
    "if \"zscore2\" not in test_runs_df.columns:\n",
    "    test_runs_df[\"zscore2\"] = False\n",
    "test_runs_df.loc[pd.isna(test_runs_df[\"zscore2\"]), \"zscore2\"] = False\n",
    "\n",
    "if \"ndim_correction\" not in test_runs_df.columns:\n",
    "    test_runs_df[\"ndim_correction\"] = False\n",
    "test_runs_df.loc[pd.isna(test_runs_df[\"ndim_correction\"]), \"ndim_correction\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6bohRRomX9gO"
   },
   "outputs": [],
   "source": [
    "config_keys = config_keys.difference(\n",
    "    {\"workers\", \"memory_avail_GB\", \"memory_total_GB\", \"memory_slurm\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 895
    },
    "id": "k_KrOF5GNj9l",
    "outputId": "6ee3be81-2a8a-4265-9fe3-44ddb591f3e3"
   },
   "outputs": [],
   "source": [
    "test_runs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Oc999K7NoA9",
    "outputId": "9ba1757f-d521-442c-bcbe-d94c57f9f100"
   },
   "outputs": [],
   "source": [
    "list(test_runs_df[\"dataset_name\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_cBYVKo7PZFY"
   },
   "outputs": [],
   "source": [
    "metric_key = \"AMI\"\n",
    "show_pc = True\n",
    "show_fmt = \"{:5.1f}\"\n",
    "eps = 0.001\n",
    "override_fields = {\n",
    "    # \"aggclust_dist_thresh\": None,  # to flip between unknown/known n clusters for AC\n",
    "}\n",
    "BEST_PARAMS = BEST_PARAMS_v1\n",
    "\n",
    "# KMeans  AffinityPropagation  AgglomerativeClustering  HDBSCAN\n",
    "clusterer = \"AgglomerativeClustering\"\n",
    "\n",
    "best_results = {k: [] for k in TEST_DATASETS}\n",
    "for dummy in [True, False]:\n",
    "    cmds = []\n",
    "    latex_table = r\"% Results for \" + f\"{metric_key}, {clusterer}\" + \"\\n\"\n",
    "    now_str = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    latex_table += r\"% Generated \" + now_str + \"\\n\"\n",
    "    latex_table += r\"\\label{tab:\" + clusterer + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"\\resizebox{\\textwidth}{!}{%\" + \"\\n\"\n",
    "    latex_table += r\"\\begin{tabular}{ll\" + r\"r\" * len(TEST_DATASETS) + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    latex_table += r\"& \" + f\"{'Encoder':<11s}\"\n",
    "    for dataset in TEST_DATASETS:\n",
    "        latex_table += r\"&\" + \"{:^15s}\".format(DATASET2SH.get(dataset, dataset))\n",
    "    latex_table += r\"\\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    for i_group, model_group_name in enumerate(list(MODEL_GROUPS.keys())):\n",
    "        if i_group > 0:\n",
    "            latex_table += r\"\\midrule\" + \"\\n\"\n",
    "        for i_model, model in enumerate(MODEL_GROUPS[model_group_name]):\n",
    "            if i_model == 0:\n",
    "                latex_table += (\n",
    "                    r\"\\parbox[t]{2mm}{\\multirow{5}{*}{\\rotatebox[origin=c]{90}{\"\n",
    "                    + model_group_name\n",
    "                    + \"}}}\"\n",
    "                )\n",
    "                latex_table += \"\\n\"\n",
    "            latex_table += f\"& {MODEL2SH.get(model, model):<10s}\"\n",
    "            for i_dataset, dataset in enumerate(TEST_DATASETS):\n",
    "                latex_table += \" &\"\n",
    "                filter = {\n",
    "                    \"model\": model,\n",
    "                    \"dataset\": dataset,\n",
    "                    \"clusterer\": clusterer,\n",
    "                }\n",
    "                sdf = select_rows(test_runs_df, filter, allow_missing=False)\n",
    "                filter2 = dict(DEFAULT_PARAMS[\"all\"], **BEST_PARAMS[clusterer][model])\n",
    "                filter2 = {k: v for k, v in filter2.items() if k not in filter}\n",
    "                filter2.update(override_fields)\n",
    "                sdf = select_rows(sdf, filter2, allow_missing=False)\n",
    "                if len(sdf) < 1:\n",
    "                    print(f\"No data for {filter} {filter2}\")\n",
    "                    if clusterer == \"AffinityPropagation\" and dataset in [\n",
    "                        \"imagenet\",\n",
    "                        \"inaturalist\",\n",
    "                    ]:\n",
    "                        continue\n",
    "                        pass\n",
    "                    cmds.append(filter2command(filter, filter2, partition=\"test\"))\n",
    "                    continue\n",
    "                if len(sdf) > 1:\n",
    "                    perf = sdf.iloc[0][\"AMI\"]\n",
    "                    if sum(sdf[\"AMI\"] != perf) > 0:\n",
    "                        print()\n",
    "                        print(\"More than one result with AMIs:\", list(sdf[\"AMI\"]))\n",
    "                        print(f\"for search {filter}\\nand {filter2}\")\n",
    "                        dif_cols = find_differing_columns(sdf, config_keys)\n",
    "                        print(f\"columns which differ: {dif_cols}\")\n",
    "                        if dif_cols:\n",
    "                            for col in dif_cols:\n",
    "                                print(f\"  {col}: {list(sdf[col])}\")\n",
    "                my_val = np.median(sdf[metric_key])\n",
    "                if dummy:\n",
    "                    best_results[dataset].append(my_val)\n",
    "                    continue\n",
    "                is_best = my_val + eps >= np.max(best_results[dataset])\n",
    "                if len(best_results[dataset]) > 1:\n",
    "                    is_secd = my_val + eps >= np.sort(best_results[dataset])[-2]\n",
    "                else:\n",
    "                    is_secd = False\n",
    "                if show_pc:\n",
    "                    my_val = my_val * 100\n",
    "                latex_table += \" $\"\n",
    "                if is_best:\n",
    "                    latex_table += r\"\\tcf{\"\n",
    "                elif is_secd:\n",
    "                    latex_table += r\"\\tcs{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                latex_table += show_fmt.format(my_val)\n",
    "                latex_table += r\"}\" if is_best or is_secd else \" \"\n",
    "                latex_table += \"$\"\n",
    "            latex_table += r\" \\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\bottomrule\" + \"\\n\"\n",
    "    latex_table += r\"\\end{tabular}\" + \"\\n\"\n",
    "    latex_table += r\"}\" + \"\\n\"\n",
    "\n",
    "\n",
    "if len(cmds) > 0:\n",
    "    print()\n",
    "for cmd in cmds:\n",
    "    print(cmd)\n",
    "\n",
    "print()\n",
    "print(\"Done!\")\n",
    "print()\n",
    "print(f\"Here is your results table for {clusterer}:\")\n",
    "print()\n",
    "print()\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZdYE4MHNNgdq"
   },
   "outputs": [],
   "source": [
    "filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qfLslVbWNlik"
   },
   "outputs": [],
   "source": [
    "filter2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5tN8jYm5NllL"
   },
   "outputs": [],
   "source": [
    "sdf = select_rows(test_runs_df, filter, allow_missing=False)\n",
    "sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xZ0tlWN5NsD6"
   },
   "outputs": [],
   "source": [
    "ff = {\n",
    "    #    'dim_reducer': 'None',\n",
    "    #    'dim_reducer_man': 'UMAP',\n",
    "    #    'zscore': False,\n",
    "    #    'normalize': False,\n",
    "    #    'zscore2': False,\n",
    "    #    'ndim_correction': False,\n",
    "    #    'distance_metric': 'chebyshev',\n",
    "    #    'aggclust_linkage': 'average',\n",
    "    #    'ndim_reduced_man': 50,\n",
    "    \"aggclust_dist_thresh\": 2.0,\n",
    "}\n",
    "select_rows(sdf, ff, allow_missing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x7Xghxw9OyUx"
   },
   "outputs": [],
   "source": [
    "sdf[\"aggclust_dist_thresh\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uppYSvUUOsR0"
   },
   "outputs": [],
   "source": [
    "\"aggclust_dist_thresh\" in sdf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-9lxUOUuhaj"
   },
   "source": [
    "## Grouping by encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n8sCItDK_2VT"
   },
   "outputs": [],
   "source": [
    "metric_key = \"AMI\"\n",
    "show_pc = True\n",
    "show_fmt = \"{:4.0f}\"\n",
    "eps = 0.001\n",
    "override_fields = {\n",
    "    # \"aggclust_dist_thresh\": None,  # to flip between unknown/known n clusters for AC\n",
    "}\n",
    "BEST_PARAMS = BEST_PARAMS_v1\n",
    "\n",
    "backbone = \"ResNet-50\"\n",
    "\n",
    "CLUSTERERS = [\n",
    "    \"KMeans\",\n",
    "    \"AgglomerativeClustering\",\n",
    "    \"AgglomerativeClustering\",\n",
    "    \"AffinityPropagation\",\n",
    "    \"HDBSCAN\",\n",
    "]\n",
    "print(MODEL2SH)\n",
    "\n",
    "best_results = {k: [] for k in TEST_DATASETS}\n",
    "for dummy in [True, False]:\n",
    "    cmds = []\n",
    "    latex_table = r\"% Results for \" + f\"{metric_key}, {backbone}\" + \"\\n\"\n",
    "    now_str = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    latex_table += r\"% Generated \" + now_str + \"\\n\"\n",
    "    latex_table += r\"\\label{tab:\" + backbone + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"\\resizebox{\\textwidth}{!}{%\" + \"\\n\"\n",
    "    latex_table += r\"\\begin{tabular}{ll\" + r\"r\" * len(TEST_DATASETS) + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    latex_table += r\"& \" + f\"{'Clusterer':<11s}\"\n",
    "    for dataset in TEST_DATASETS:\n",
    "        latex_table += r\"&\" + \"{:^15s}\".format(DATASET2SH.get(dataset, dataset))\n",
    "    latex_table += r\"\\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    print(MODEL_GROUPS[backbone])\n",
    "    for i_group, model in enumerate(list(MODEL_GROUPS[backbone])):\n",
    "        print(model)\n",
    "        if i_group > 0:\n",
    "            latex_table += r\"\\midrule\" + \"\\n\"\n",
    "\n",
    "        first_agg = True\n",
    "        for i_clusters, clusterer in enumerate(CLUSTERERS):\n",
    "            if i_clusters == 0:\n",
    "                latex_table += (\n",
    "                    r\"\\parbox[t]{2mm}{\\multirow{5}{*}{\\rotatebox[origin=c]{90}{\"\n",
    "                    + MODEL2SH[model]\n",
    "                    + \"}}}\"\n",
    "                )\n",
    "                latex_table += \"\\n\"\n",
    "            override_fields = {}\n",
    "            clusterername = CLUSTERER2SH.get(clusterer, clusterer)\n",
    "            if first_agg and clusterer == \"AgglomerativeClustering\":\n",
    "                first_agg = False\n",
    "                override_fields = {\"aggclust_dist_thresh\": None}\n",
    "                clusterername = \"AC  w/ C\"\n",
    "            elif clusterer == \"AgglomerativeClustering\":\n",
    "                clusterername = \"AC w/o C\"\n",
    "            latex_table += f\"& {clusterername:<10s}\"\n",
    "            for i_dataset, dataset in enumerate(TEST_DATASETS):\n",
    "                latex_table += \" &\"\n",
    "                filter = {\n",
    "                    \"model\": model,\n",
    "                    \"dataset\": dataset,\n",
    "                    \"clusterer\": clusterer,\n",
    "                }\n",
    "                sdf = select_rows(test_runs_df, filter, allow_missing=False)\n",
    "                filter2 = dict(DEFAULT_PARAMS[\"all\"], **BEST_PARAMS[clusterer][model])\n",
    "                filter2 = {k: v for k, v in filter2.items() if k not in filter}\n",
    "                filter2.update(override_fields)\n",
    "                sdf = select_rows(sdf, filter2, allow_missing=False)\n",
    "                if len(sdf) < 1:\n",
    "                    print(f\"No data for {filter} {filter2}\")\n",
    "                    cmds.append(filter2command(filter, filter2, partition=\"test\"))\n",
    "                    continue\n",
    "                if len(sdf) > 1:\n",
    "                    perf = sdf.iloc[0][\"AMI\"]\n",
    "                    if sum(sdf[\"AMI\"] != perf) > 0:\n",
    "                        print()\n",
    "                        print(\"More than one result with AMIs:\", list(sdf[\"AMI\"]))\n",
    "                        print(f\"for search {filter}\\nand {filter2}\")\n",
    "                        dif_cols = find_differing_columns(sdf, config_keys)\n",
    "                        print(f\"columns which differ: {dif_cols}\")\n",
    "                        if dif_cols:\n",
    "                            for col in dif_cols:\n",
    "                                print(f\"  {col}: {list(sdf[col])}\")\n",
    "                my_val = np.median(sdf[metric_key])\n",
    "                if dummy:\n",
    "                    best_results[dataset].append(my_val)\n",
    "                    continue\n",
    "                is_best = my_val + eps >= np.max(best_results[dataset])\n",
    "                if len(best_results[dataset]) > 1:\n",
    "                    is_secd = my_val + eps >= np.sort(best_results[dataset])[-2]\n",
    "                else:\n",
    "                    is_secd = False\n",
    "                if show_pc:\n",
    "                    my_val = my_val * 100\n",
    "                latex_table += \" $\"\n",
    "                if is_best:\n",
    "                    latex_table += r\"\\tcf{\"\n",
    "                elif is_secd:\n",
    "                    latex_table += r\"\\tcs{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                latex_table += show_fmt.format(my_val)\n",
    "                latex_table += r\"}\" if is_best or is_secd else \" \"\n",
    "                latex_table += \"$\"\n",
    "            latex_table += r\" \\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\bottomrule\" + \"\\n\"\n",
    "    latex_table += r\"\\end{tabular}\" + \"\\n\"\n",
    "    latex_table += r\"}\" + \"\\n\"\n",
    "\n",
    "\n",
    "if len(cmds) > 0:\n",
    "    print()\n",
    "for cmd in cmds:\n",
    "    print(cmd)\n",
    "\n",
    "print()\n",
    "print(\"Done!\")\n",
    "print()\n",
    "print(f\"Here is your results table for {clusterer}:\")\n",
    "print()\n",
    "print()\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_isFUjNsuYc4"
   },
   "source": [
    "## Grouping by clusterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UkhJKXC5p7GE"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MyWQRN9iqBkI",
    "outputId": "8515de9f-79f8-48aa-8ee4-a572274df148"
   },
   "outputs": [],
   "source": [
    "metric_key = \"AMI\"  # AMI  num_cluster_pred  silhouette-euclidean_pred  silhouette-og-euclidean_pred\n",
    "show_pc = True\n",
    "show_fmt = \"{:4.0f}\"\n",
    "highlight_best = True\n",
    "use_si_num = False\n",
    "eps = 0.005\n",
    "override_fields = {\n",
    "    # \"aggclust_dist_thresh\": None,  # to flip between unknown/known n clusters for AC\n",
    "}\n",
    "BEST_PARAMS = BEST_PARAMS_v1\n",
    "\n",
    "backbone = \"ViT-B\"  # \"ResNet-50\" or \"ViT-B\"\n",
    "\n",
    "if metric_key == \"num_cluster_pred\":\n",
    "    CLUSTERERS = [\"AgglomerativeClustering\", \"AffinityPropagation\", \"HDBSCAN\"]\n",
    "    show_pc = False\n",
    "    show_fmt = \"{:4.0f}\"\n",
    "    highlight_best = False\n",
    "    use_si_num = True\n",
    "    override_fields = {}\n",
    "else:\n",
    "    CLUSTERERS = [\n",
    "        \"KMeans\",\n",
    "        \"AgglomerativeClustering\",\n",
    "        \"AgglomerativeClustering\",\n",
    "        \"AffinityPropagation\",\n",
    "        \"HDBSCAN\",\n",
    "    ]\n",
    "if metric_key.startswith(\"silhouette\"):\n",
    "    show_pc = False\n",
    "    show_fmt = \"{:5.2f}\"\n",
    "\n",
    "print(MODEL2SH)\n",
    "\n",
    "best_results = {k: [] for k in TEST_DATASETS}\n",
    "best_results_grouped = {k: defaultdict(lambda: []) for k in TEST_DATASETS}\n",
    "\n",
    "for dummy in [True, False]:\n",
    "    cmds = []\n",
    "    latex_table = r\"% Results for \" + f\"{metric_key}, {backbone}\" + \"\\n\"\n",
    "    now_str = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    latex_table += r\"% Generated \" + now_str + \"\\n\"\n",
    "    label = backbone\n",
    "    if metric_key == \"AMI\":\n",
    "        latex_table += r\"\\label{tab:\" + label + r\"}\" + \"\\n\"\n",
    "    label = metric_key.replace(\"_\", \"-\") + \":\" + label\n",
    "    latex_table += r\"\\label{tab:\" + label + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"\\resizebox{\\textwidth}{!}{%\" + \"\\n\"\n",
    "    latex_table += r\"\\begin{tabular}{ll\" + r\"r\" * len(TEST_DATASETS) + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    latex_table += r\"& \" + f\"{'Encoder':<11s}\"\n",
    "    for dataset in TEST_DATASETS:\n",
    "        latex_table += r\"&\" + \"{:^15s}\".format(DATASET2SH.get(dataset, dataset))\n",
    "    latex_table += r\"\\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    print(MODEL_GROUPS[backbone])\n",
    "    if metric_key == \"num_cluster_pred\":\n",
    "        latex_table += r\"& Num targets\"\n",
    "        for i_dataset, dataset in enumerate(TEST_DATASETS):\n",
    "            sdf = select_rows(test_runs_df, {\"dataset\": dataset}, allow_missing=False)\n",
    "            sdf = sdf[~pd.isna(sdf[\"num_cluster_true\"])]\n",
    "            latex_table += r\"& \"\n",
    "            latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "            latex_table += f\"{sdf.iloc[0]['num_cluster_true'].item()}\"\n",
    "            latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "        latex_table += r\"\\\\\" + \"\\n\"\n",
    "        latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    elif metric_key.endswith(\"_pred\"):\n",
    "        metric_key2 = metric_key.replace(\"_pred\", \"_true\")\n",
    "        clusterername = \"G.T.\"\n",
    "        latex_table += (\n",
    "            r\"\\parbox[t]{2mm}{\\multirow{5}{*}{\\rotatebox[origin=c]{90}{\"\n",
    "            + clusterername\n",
    "            + \"}}}\"\n",
    "        )\n",
    "        latex_table += \"\\n\"\n",
    "        for i_group, model in enumerate(list(MODEL_GROUPS[backbone])):\n",
    "            latex_table += f\"& {MODEL2SH[model]:<10s}\"\n",
    "            for i_dataset, dataset in enumerate(TEST_DATASETS):\n",
    "                latex_table += \" &\"\n",
    "                filter = {\"model\": model, \"dataset\": dataset}\n",
    "                if model == \"timm_vit_base_patch16_224.mae\":\n",
    "                    filter[\"dim_reducer\"] = \"PCA\"\n",
    "                    filter[\"pca_variance\"] = 0.95\n",
    "                else:\n",
    "                    filter[\"dim_reducer_man\"] = \"UMAP\"\n",
    "                    filter[\"ndim_reduced_man\"] = 50\n",
    "                    filter[\"dim_reducer_man_metric\"] = \"euclidean\"\n",
    "                sdf = select_rows(test_runs_df, filter, allow_missing=False)\n",
    "                sdf = sdf[~pd.isna(sdf[metric_key2])]\n",
    "                my_val = np.nanmedian(sdf[metric_key])\n",
    "                if sum(sdf[metric_key2] != my_val) > 0:\n",
    "                    pass\n",
    "                if dummy:\n",
    "                    best_results_grouped[dataset][clusterername].append(my_val)\n",
    "                    continue\n",
    "                is_best_grp = my_val + eps >= np.max(\n",
    "                    best_results_grouped[dataset][clusterername]\n",
    "                )\n",
    "                latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "                latex_table += \"     \"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best_grp:\n",
    "                    latex_table += r\"\\tcg{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                latex_table += show_fmt.format(my_val)\n",
    "                if highlight_best:\n",
    "                    latex_table += r\"}\" if is_best_grp else \" \"\n",
    "                latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "\n",
    "            latex_table += r\" \\\\\" + \"\\n\"\n",
    "        latex_table += r\"\\toprule\" + \"\\n\"\n",
    "\n",
    "    first_agg = True\n",
    "    for i_clusters, clusterer in enumerate(CLUSTERERS):\n",
    "        override_fields = {}\n",
    "        clusterername = CLUSTERER2SH.get(clusterer, clusterer)\n",
    "        if (\n",
    "            first_agg\n",
    "            and clusterer == \"AgglomerativeClustering\"\n",
    "            and metric_key != \"num_cluster_pred\"\n",
    "        ):\n",
    "            first_agg = False\n",
    "            override_fields = {\"aggclust_dist_thresh\": None}\n",
    "            clusterername = \"AC  w/ C\"\n",
    "        elif clusterer == \"AgglomerativeClustering\":\n",
    "            clusterername = \"AC w/o C\"\n",
    "\n",
    "        if i_clusters > 0:\n",
    "            latex_table += r\"\\midrule\" + \"\\n\"\n",
    "\n",
    "        latex_table += (\n",
    "            r\"\\parbox[t]{2mm}{\\multirow{5}{*}{\\rotatebox[origin=c]{90}{\"\n",
    "            + clusterername\n",
    "            + \"}}}\"\n",
    "        )\n",
    "        latex_table += \"\\n\"\n",
    "\n",
    "        for i_group, model in enumerate(list(MODEL_GROUPS[backbone])):\n",
    "            print(model)\n",
    "\n",
    "            latex_table += f\"& {MODEL2SH[model]:<10s}\"\n",
    "            for i_dataset, dataset in enumerate(TEST_DATASETS):\n",
    "                latex_table += \" &\"\n",
    "                filter = {\n",
    "                    \"model\": model,\n",
    "                    \"dataset\": dataset,\n",
    "                    \"clusterer\": clusterer,\n",
    "                }\n",
    "                sdf = select_rows(test_runs_df, filter, allow_missing=False)\n",
    "                filter2 = dict(DEFAULT_PARAMS[\"all\"], **BEST_PARAMS[clusterer][model])\n",
    "                filter2 = {k: v for k, v in filter2.items() if k not in filter}\n",
    "                filter2.update(override_fields)\n",
    "                sdf = select_rows(sdf, filter2, allow_missing=False)\n",
    "                if len(sdf) < 1:\n",
    "                    print(f\"No data for {filter} {filter2}\")\n",
    "                    cmds.append(filter2command(filter, filter2, partition=\"test\"))\n",
    "                    if not dummy:\n",
    "                        # latex_table += r\"\\multicolumn{1}{c}{--}\"\n",
    "                        latex_table += r\"   --  \"\n",
    "                    continue\n",
    "                if len(sdf) > 1:\n",
    "                    perf = sdf.iloc[0][\"AMI\"]\n",
    "                    if sum(sdf[\"AMI\"] != perf) > 0:\n",
    "                        print()\n",
    "                        print(\"More than one result with AMIs:\", list(sdf[\"AMI\"]))\n",
    "                        print(f\"for search {filter}\\nand {filter2}\")\n",
    "                        dif_cols = find_differing_columns(sdf, config_keys)\n",
    "                        print(f\"columns which differ: {dif_cols}\")\n",
    "                        if dif_cols:\n",
    "                            for col in dif_cols:\n",
    "                                print(f\"  {col}: {list(sdf[col])}\")\n",
    "                my_val = np.nanmedian(sdf[metric_key])\n",
    "                if dummy:\n",
    "                    best_results[dataset].append(my_val)\n",
    "                    best_results_grouped[dataset][clusterername].append(my_val)\n",
    "                    continue\n",
    "                if np.isnan(my_val):\n",
    "                    latex_table += r\"   --  \"\n",
    "                    continue\n",
    "                is_best = my_val + eps >= np.max(best_results[dataset])\n",
    "                if len(best_results[dataset]) > 1:\n",
    "                    is_secd = my_val + eps >= np.sort(best_results[dataset])[-2]\n",
    "                else:\n",
    "                    is_secd = False\n",
    "                is_best_grp = my_val + eps >= np.max(\n",
    "                    best_results_grouped[dataset][clusterername]\n",
    "                )\n",
    "                if show_pc:\n",
    "                    my_val = my_val * 100\n",
    "                latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best:\n",
    "                    latex_table += r\"\\tcf{\"\n",
    "                elif is_secd:\n",
    "                    latex_table += r\"\\tcs{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best_grp:\n",
    "                    latex_table += r\"\\tcg{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                latex_table += show_fmt.format(my_val)\n",
    "                if highlight_best:\n",
    "                    latex_table += r\"}\" if is_best or is_secd else \" \"\n",
    "                    latex_table += r\"}\" if is_best_grp else \" \"\n",
    "                latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "            latex_table += r\" \\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\bottomrule\" + \"\\n\"\n",
    "    latex_table += r\"\\end{tabular}\" + \"\\n\"\n",
    "    latex_table += r\"}\" + \"\\n\"\n",
    "\n",
    "\n",
    "if len(cmds) > 0:\n",
    "    print()\n",
    "for cmd in cmds:\n",
    "    print(cmd)\n",
    "\n",
    "print()\n",
    "print(\"Done!\")\n",
    "print()\n",
    "print(f\"Here is your results table for {metric_key}, {backbone}:\")\n",
    "print()\n",
    "print()\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWpBwhP8MOm0"
   },
   "source": [
    "## Correlation between AMI and SIlhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lU7C4-fBpl5R"
   },
   "outputs": [],
   "source": [
    "best_results_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gp5BaVDxG2Il"
   },
   "outputs": [],
   "source": [
    "metric_key1 = \"AMI\"\n",
    "metric_key2 = \"silhouette-euclidean_pred\"\n",
    "BEST_PARAMS = BEST_PARAMS_v1\n",
    "\n",
    "\n",
    "CLUSTERERS = [\n",
    "    \"KMeans\",\n",
    "    \"AffinityPropagation\",\n",
    "    \"AgglomerativeClustering\",\n",
    "    \"AgglomerativeClustering\",\n",
    "    \"HDBSCAN\",\n",
    "]\n",
    "print(MODEL2SH)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, sharey=True, figsize=(5, 3))\n",
    "\n",
    "\n",
    "colors = [\n",
    "    \"tab:blue\",\n",
    "    \"tab:orange\",\n",
    "    \"tab:green\",\n",
    "    \"tab:red\",\n",
    "    \"tab:purple\",\n",
    "    \"tab:brown\",\n",
    "    \"tab:pink\",\n",
    "    \"tab:gray\",\n",
    "    \"tab:olive\",\n",
    "    \"tab:cyan\",\n",
    "]\n",
    "\n",
    "correlations = {\"ResNet-50\": [], \"ViT-B\": []}\n",
    "for i_backbone, backbone in enumerate([\"ResNet-50\", \"ViT-B\"]):\n",
    "    my_valx_overall = []\n",
    "    my_valy_overall = []\n",
    "\n",
    "    my_valx_method = {clusterer: [] for clusterer in CLUSTERERS}\n",
    "    my_valy_method = {clusterer: [] for clusterer in CLUSTERERS}\n",
    "    best_results = {k: [] for k in TEST_DATASETS}\n",
    "\n",
    "    for i_dataset, dataset in enumerate(TEST_DATASETS):\n",
    "        my_valx = []\n",
    "        my_valy = []\n",
    "        first_agg = True\n",
    "        for i_clusters, clusterer in enumerate(CLUSTERERS):\n",
    "            clusterername = clusterer\n",
    "            if first_agg and clusterer == \"AgglomerativeClustering\":\n",
    "                first_agg = False\n",
    "                override_fields = {\"aggclust_dist_thresh\": None}\n",
    "                clusterername = \"AC  w/ C\"\n",
    "            elif clusterer == \"AgglomerativeClustering\":\n",
    "                override_fields = {}\n",
    "                clusterername = \"AC w/o C\"\n",
    "\n",
    "            for i_group, model in enumerate(list(MODEL_GROUPS[backbone])):\n",
    "                if i_group == 0:\n",
    "                    latex_table += (\n",
    "                        r\"\\parbox[t]{2mm}{\\multirow{5}{*}{\\rotatebox[origin=c]{90}{\"\n",
    "                        + clusterername\n",
    "                        + \"}}}\"\n",
    "                    )\n",
    "                    latex_table += \"\\n\"\n",
    "\n",
    "                latex_table += f\"& {MODEL2SH[model]:<10s}\"\n",
    "                latex_table += \" &\"\n",
    "                filter = {\n",
    "                    \"model\": model,\n",
    "                    \"dataset\": dataset,\n",
    "                    \"clusterer\": clusterer,\n",
    "                }\n",
    "                sdf = select_rows(test_runs_df, filter, allow_missing=False)\n",
    "                filter2 = dict(DEFAULT_PARAMS[\"all\"], **BEST_PARAMS[clusterer][model])\n",
    "                filter2 = {k: v for k, v in filter2.items() if k not in filter}\n",
    "                filter2.update(override_fields)\n",
    "                sdf = select_rows(sdf, filter2, allow_missing=False)\n",
    "                if len(sdf) < 1:\n",
    "                    cmds.append(filter2command(filter, filter2, partition=\"test\"))\n",
    "                    continue\n",
    "                my_valx.append(np.nanmedian(sdf[metric_key1]))\n",
    "                my_valy.append(np.nanmedian(sdf[metric_key2]))\n",
    "\n",
    "                my_valx_method[clusterer].append(np.nanmedian(sdf[metric_key1]))\n",
    "                my_valy_method[clusterer].append(np.nanmedian(sdf[metric_key2]))\n",
    "\n",
    "        correlations[backbone].append(np.corrcoef(my_valx, my_valy)[0, 1])\n",
    "\n",
    "        ax[i_backbone].scatter(\n",
    "            my_valy,\n",
    "            my_valx,\n",
    "            color=colors[i_dataset],\n",
    "            alpha=0.5,\n",
    "            label=TEST_DATASETS[i_dataset],\n",
    "        )\n",
    "        my_valx_overall.extend(my_valx)\n",
    "        my_valy_overall.extend(my_valy)\n",
    "        ax[i_backbone].set_xlabel(r\"$S$\")\n",
    "        if i_backbone == 0:\n",
    "            ax[i_backbone].set_ylabel(metric_key1)\n",
    "        ax[i_backbone].set_ylim(-0.05, 1.05)\n",
    "        ax[i_backbone].set_xlim(-1.05, 1.05)\n",
    "        ax[i_backbone].set_title(\n",
    "            f\"{backbone}\\nPCC: {np.corrcoef(my_valx_overall, my_valy_overall)[0,1]:.2f}\"\n",
    "        )\n",
    "\n",
    "\n",
    "label_fn = lambda c, marker: plt.plot(  # noqa:E731\n",
    "    [], [], color=c, ls=\"None\", marker=marker, linewidth=6\n",
    ")[0]\n",
    "handles = [label_fn(colors[idx], \"o\") for idx in range(len(TEST_DATASETS))]\n",
    "data_labels = [DATASET2SH.get(dataset, dataset) for dataset in TEST_DATASETS]\n",
    "\n",
    "ax[1].legend(handles, data_labels, loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "print(data_labels)\n",
    "print(correlations[\"ResNet-50\"], len(correlations[\"ResNet-50\"]))\n",
    "print(correlations[\"ViT-B\"], len(correlations[\"ViT-B\"]))\n",
    "\n",
    "fig.savefig(\"ami_silhouette.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eSlyPO1kbM2u"
   },
   "outputs": [],
   "source": [
    "metric_key1 = \"AMI\"\n",
    "metric_key2 = \"silhouette-og-euclidean_pred\"\n",
    "BEST_PARAMS = BEST_PARAMS_v1\n",
    "\n",
    "CLUSTERERS = [\n",
    "    \"KMeans\",\n",
    "    \"AffinityPropagation\",\n",
    "    \"AgglomerativeClustering\",\n",
    "    \"AgglomerativeClustering\",\n",
    "    \"HDBSCAN\",\n",
    "]\n",
    "print(MODEL2SH)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, sharey=True, figsize=(5.5, 3))\n",
    "\n",
    "colors = [\n",
    "    \"tab:red\",\n",
    "    \"tab:blue\",\n",
    "    \"tab:orange\",\n",
    "    \"tab:green\",\n",
    "    \"tab:purple\",\n",
    "    \"tab:brown\",\n",
    "    \"tab:pink\",\n",
    "    \"tab:gray\",\n",
    "    \"tab:olive\",\n",
    "    \"tab:cyan\",\n",
    "]\n",
    "\n",
    "correlations = {\"ResNet-50\": [], \"ViT-B\": []}\n",
    "for i_backbone, backbone in enumerate([\"ResNet-50\", \"ViT-B\"]):\n",
    "    my_valx_overall = []\n",
    "    my_valy_overall = []\n",
    "    best_results = {k: [] for k in TEST_DATASETS}\n",
    "\n",
    "    for i_dataset, dataset in enumerate(TEST_DATASETS):\n",
    "        my_valx = []\n",
    "        my_valy = []\n",
    "        first_agg = True\n",
    "        for i_clusters, clusterer in enumerate(CLUSTERERS):\n",
    "            clusterername = clusterer\n",
    "            if first_agg and clusterer == \"AgglomerativeClustering\":\n",
    "                first_agg = False\n",
    "                override_fields = {\"aggclust_dist_thresh\": None}\n",
    "                clusterername = \"AC  w/ C\"\n",
    "            elif clusterer == \"AgglomerativeClustering\":\n",
    "                override_fields = {}\n",
    "                clusterername = \"AC w/o C\"\n",
    "\n",
    "            for i_group, model in enumerate(list(MODEL_GROUPS[backbone])):\n",
    "                if i_group == 0:\n",
    "                    latex_table += (\n",
    "                        r\"\\parbox[t]{2mm}{\\multirow{5}{*}{\\rotatebox[origin=c]{90}{\"\n",
    "                        + clusterername\n",
    "                        + \"}}}\"\n",
    "                    )\n",
    "                    latex_table += \"\\n\"\n",
    "\n",
    "                latex_table += f\"& {MODEL2SH[model]:<10s}\"\n",
    "                latex_table += \" &\"\n",
    "                filter = {\n",
    "                    \"model\": model,\n",
    "                    \"dataset\": dataset,\n",
    "                    \"clusterer\": clusterer,\n",
    "                }\n",
    "                sdf = select_rows(test_runs_df, filter, allow_missing=False)\n",
    "                filter2 = dict(DEFAULT_PARAMS[\"all\"], **BEST_PARAMS[clusterer][model])\n",
    "                filter2 = {k: v for k, v in filter2.items() if k not in filter}\n",
    "                filter2.update(override_fields)\n",
    "                sdf = select_rows(sdf, filter2, allow_missing=False)\n",
    "                if len(sdf) < 1:\n",
    "                    cmds.append(filter2command(filter, filter2, partition=\"test\"))\n",
    "                    continue\n",
    "                my_valx.append(np.nanmedian(sdf[metric_key1]))\n",
    "                my_valy.append(np.nanmedian(sdf[metric_key2]))\n",
    "\n",
    "        correlations[backbone].append(np.corrcoef(my_valx, my_valy)[0, 1])\n",
    "\n",
    "        ax[i_backbone].scatter(\n",
    "            my_valy,\n",
    "            my_valx,\n",
    "            color=colors[i_dataset],\n",
    "            alpha=0.5,\n",
    "            s=8,\n",
    "            label=TEST_DATASETS[i_dataset],\n",
    "        )\n",
    "        my_valx_overall.extend(my_valx)\n",
    "        my_valy_overall.extend(my_valy)\n",
    "\n",
    "    ax[i_backbone].set_xlabel(r\"$S$\")\n",
    "    if i_backbone == 0:\n",
    "        ax[i_backbone].set_ylabel(metric_key1)\n",
    "    ax[i_backbone].set_ylim(-0.05, 1.05)\n",
    "    ax[i_backbone].set_xlim(-1.05, 1.05)\n",
    "    ax[i_backbone].set_title(backbone)\n",
    "    my_valx_overall = np.array(my_valx_overall)\n",
    "    my_valy_overall = np.array(my_valy_overall)\n",
    "    select = ~(np.isnan(my_valx_overall) | np.isnan(my_valy_overall))\n",
    "    cor = np.corrcoef(my_valx_overall[select], my_valy_overall[select])\n",
    "    ax[i_backbone].text(-0.85, 0.95, f\"$r={cor[0,1]:.2f}$\")\n",
    "\n",
    "\n",
    "label_fn = lambda c, marker: plt.plot(  # noqa:E731\n",
    "    [], [], color=c, ls=\"None\", marker=marker, linewidth=6\n",
    ")[0]\n",
    "handles = [label_fn(colors[idx], \"o\") for idx in range(len(TEST_DATASETS))]\n",
    "data_labels = [DATASET2SH.get(dataset, dataset) for dataset in TEST_DATASETS]\n",
    "\n",
    "ax[1].legend(handles, data_labels, loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "print(data_labels)\n",
    "print(correlations[\"ResNet-50\"], len(correlations[\"ResNet-50\"]))\n",
    "print(correlations[\"ViT-B\"], len(correlations[\"ViT-B\"]))\n",
    "\n",
    "fig.savefig(\n",
    "    f\"{metric_key1}_{metric_key2.replace('-euclidean', '')}.pdf\", bbox_inches=\"tight\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZCuo294WiED8"
   },
   "outputs": [],
   "source": [
    "cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MuURKU9mMXIN"
   },
   "outputs": [],
   "source": [
    "metric_key1 = \"AMI\"\n",
    "metric_key2 = \"silhouette-euclidean_pred\"\n",
    "BEST_PARAMS = BEST_PARAMS_v1\n",
    "\n",
    "CLUSTERERS = [\n",
    "    \"KMeans\",\n",
    "    \"AffinityPropagation\",\n",
    "    \"AgglomerativeClustering\",\n",
    "    \"AgglomerativeClustering\",\n",
    "    \"HDBSCAN\",\n",
    "]\n",
    "print(MODEL2SH)\n",
    "\n",
    "figenc, axenc = plt.subplots(1, 2, figsize=(6, 2))\n",
    "figclus, axclus = plt.subplots(1, 2, figsize=(6, 2))\n",
    "\n",
    "for i_backbone, backbone in enumerate([\"ResNet-50\", \"ViT-B\"]):\n",
    "    result_table = np.zeros(\n",
    "        (5, len(CLUSTERERS), len(TEST_DATASETS))\n",
    "    )  # Encoders, clusteres, dataset\n",
    "    for dummy in [True, False]:\n",
    "        cmds = []\n",
    "\n",
    "        for i_group, model in enumerate(list(MODEL_GROUPS[backbone])):\n",
    "            first_agg = True\n",
    "            for i_clusters, clusterer in enumerate(CLUSTERERS):\n",
    "                clusterername = clusterer\n",
    "                if first_agg and clusterer == \"AgglomerativeClustering\":\n",
    "                    first_agg = False\n",
    "                    override_fields = {\"aggclust_dist_thresh\": None}\n",
    "                    clusterername = \"Agg  w/ C\"\n",
    "                elif clusterer == \"AgglomerativeClustering\":\n",
    "                    override_fields = {}\n",
    "                    clusterername = \"Agg w/o C\"\n",
    "\n",
    "                for i_dataset, dataset in enumerate(TEST_DATASETS):\n",
    "                    latex_table += \" &\"\n",
    "                    filter = {\n",
    "                        \"model\": model,\n",
    "                        \"dataset\": dataset,\n",
    "                        \"clusterer\": clusterer,\n",
    "                    }\n",
    "                    sdf = select_rows(test_runs_df, filter, allow_missing=False)\n",
    "                    filter2 = dict(\n",
    "                        DEFAULT_PARAMS[\"all\"], **BEST_PARAMS[clusterer][model]\n",
    "                    )\n",
    "                    filter2 = {k: v for k, v in filter2.items() if k not in filter}\n",
    "                    filter2.update(override_fields)\n",
    "                    sdf = select_rows(sdf, filter2, allow_missing=False)\n",
    "                    if len(sdf) < 1:\n",
    "                        cmds.append(filter2command(filter, filter2, partition=\"test\"))\n",
    "                        result_table[i_group, i_clusters, i_dataset] = -100.0\n",
    "                        continue\n",
    "                    result_table[i_group, i_clusters, i_dataset] = np.median(\n",
    "                        sdf[metric_key1]\n",
    "                    )\n",
    "\n",
    "    print(result_table[0])\n",
    "\n",
    "    print(backbone)\n",
    "    print(MODEL_GROUPS[backbone])\n",
    "    CLUSTERERS2 = [\"K-Means\", \"Affinity Prop\", \"Agg w/ C\", \"Agg w/o C\", \"HDBSCAN\"]\n",
    "    colors = [\"tab:blue\", \"tab:orange\", \"tab:red\", \"tab:green\", \"tab:olive\", \"tab:cyan\"]\n",
    "\n",
    "    encoder_to_color = {}\n",
    "    cluster_to_color = {\n",
    "        CLUSTERERS2[idx]: colors[idx] for idx in range(len(CLUSTERERS2))\n",
    "    }\n",
    "\n",
    "    for model in list(MODEL_GROUPS[backbone]):\n",
    "        if model == \"resnet50\" or model == \"vitb16\":\n",
    "            encoder_to_color[model] = colors[0]\n",
    "        if \"mae\" in model:\n",
    "            encoder_to_color[model] = colors[1]\n",
    "        if \"vicreg\" in model:\n",
    "            encoder_to_color[model] = colors[2]\n",
    "        if \"clip\" in model:\n",
    "            encoder_to_color[model] = colors[3]\n",
    "        if \"moco\" in model:\n",
    "            encoder_to_color[model] = colors[4]\n",
    "        if \"dino\" in model:\n",
    "            encoder_to_color[model] = colors[5]\n",
    "\n",
    "    print(encoder_to_color)\n",
    "    rank_tmp = np.asarray([1, 2, 3, 4, 5])\n",
    "    # RANK PER ENCODER - go through each dataset, look at each clusterer,\n",
    "    # and determine the rank of each encoder in that setting\n",
    "    print(list(MODEL_GROUPS[backbone]))\n",
    "    ranks_encoders = np.zeros((5, len(CLUSTERERS), len(TEST_DATASETS)))\n",
    "    for i_dataset in range(len(TEST_DATASETS)):\n",
    "        for i_clusters in range(len(CLUSTERERS)):\n",
    "            cluster_data = result_table[:, i_clusters, i_dataset]\n",
    "            rank = np.argsort(cluster_data)[::-1]\n",
    "            ranks_encoders[:, i_clusters, i_dataset] = rank_tmp[rank.argsort()]\n",
    "    mean_rank_encoders = np.mean(ranks_encoders, axis=(1, 2))\n",
    "    std_rank_encoders = np.std(ranks_encoders, axis=(1, 2))\n",
    "    order = [\n",
    "        (\n",
    "            list(MODEL_GROUPS[backbone])[idx],\n",
    "            mean_rank_encoders[idx],\n",
    "            std_rank_encoders[idx],\n",
    "        )\n",
    "        for idx in np.argsort(mean_rank_encoders)\n",
    "    ]\n",
    "\n",
    "    for idx, model in enumerate(order[::-1]):\n",
    "        axenc[i_backbone].barh(\n",
    "            idx,\n",
    "            model[1],\n",
    "            xerr=model[2],\n",
    "            align=\"center\",\n",
    "            alpha=0.6,\n",
    "            ecolor=\"black\",\n",
    "            color=encoder_to_color[model[0]],\n",
    "            capsize=2,\n",
    "            zorder=10,\n",
    "        )\n",
    "\n",
    "    axenc[i_backbone].set_yticks([])\n",
    "    axenc[i_backbone].set_yticklabels([])\n",
    "    axenc[i_backbone].set_xticks([1, 2, 3, 4, 5])\n",
    "    axenc[i_backbone].set_xticklabels([1, 2, 3, 4, 5])\n",
    "    axenc[i_backbone].xaxis.grid(True, zorder=1, alpha=0.5)\n",
    "    axenc[i_backbone].set_title(f\"{backbone}\")\n",
    "\n",
    "    # RANK PER CLUSTERER - go through each dataset, look at each encoder,\n",
    "    # and determine the rank of each clusterer in that setting\n",
    "\n",
    "    print(CLUSTERERS2)\n",
    "    ranks_clusterers = np.zeros((5, len(CLUSTERERS2), len(TEST_DATASETS)))\n",
    "    for i_dataset in range(len(TEST_DATASETS)):\n",
    "        for i_encoder in range(len(list(MODEL_GROUPS[backbone]))):\n",
    "            encoder_data = result_table[i_encoder, :, i_dataset]\n",
    "            rank = np.argsort(encoder_data)[::-1]\n",
    "            ranks_clusterers[i_encoder, :, i_dataset] = rank_tmp[rank.argsort()]\n",
    "    mean_rank_clusters = np.mean(ranks_clusterers, axis=(0, 2))\n",
    "    std_rank_clusters = np.std(ranks_clusterers, axis=(0, 2))\n",
    "    order = [\n",
    "        (CLUSTERERS2[idx], mean_rank_clusters[idx], std_rank_clusters[idx])\n",
    "        for idx in np.argsort(mean_rank_clusters)\n",
    "    ]\n",
    "\n",
    "    for idx, model in enumerate(order[::-1]):\n",
    "        axclus[i_backbone].barh(\n",
    "            idx,\n",
    "            model[1],\n",
    "            xerr=model[2],\n",
    "            align=\"center\",\n",
    "            alpha=0.6,\n",
    "            ecolor=\"black\",\n",
    "            color=cluster_to_color[model[0]],\n",
    "            capsize=2,\n",
    "            zorder=10,\n",
    "        )\n",
    "\n",
    "    axclus[i_backbone].set_yticks([])\n",
    "    axclus[i_backbone].set_yticklabels([])\n",
    "    axclus[i_backbone].set_xticks([1, 2, 3, 4, 5])\n",
    "    axclus[i_backbone].set_xticklabels([1, 2, 3, 4, 5])\n",
    "    axclus[i_backbone].xaxis.grid(True, zorder=1, alpha=0.5)\n",
    "    axclus[i_backbone].set_title(f\"{backbone}\")\n",
    "\n",
    "    axclus[i_backbone].set_xlabel(\"Rank\")\n",
    "    axenc[i_backbone].set_xlabel(\"Rank\")\n",
    "\n",
    "    print(order)\n",
    "\n",
    "\n",
    "encoder_to_color[\"vicreg_resnet50\"] = colors[2]\n",
    "\n",
    "label_fn = lambda c, ls: plt.plot([], [], color=c, ls=ls, linewidth=3)[0]  # noqa:E731\n",
    "handles_clus = [label_fn(cluster_to_color[idx], \"-\") for idx in CLUSTERERS2]\n",
    "handles_enc = [\n",
    "    label_fn(encoder_to_color[idx], \"-\")\n",
    "    for idx in list(MODEL_GROUPS[backbone]) + [\"vicreg_resnet50\"]\n",
    "]\n",
    "\n",
    "axenc[1].legend(\n",
    "    handles_enc,\n",
    "    [MODEL2SH[x] for x in list(MODEL_GROUPS[backbone]) + [\"vicreg_resnet50\"]],\n",
    "    loc=\"center left\",\n",
    "    bbox_to_anchor=(1, 0.5),\n",
    ")\n",
    "axclus[1].legend(handles_clus, CLUSTERERS2, loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "figenc.savefig(\"ranking_enc.pdf\", bbox_inches=\"tight\")\n",
    "figclus.savefig(\"ranking_clus.pdf\", bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "4vS4LHHiHKCG"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
